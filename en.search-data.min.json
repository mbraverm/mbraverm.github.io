[{"id":0,"href":"/posts/","title":"News","parent":"Mark Braverman","content":""},{"id":1,"href":"/about/","title":"About","parent":"Mark Braverman","content":""},{"id":2,"href":"/about/brief-bio/","title":"Brief Bio","parent":"About","content":"I received my PhD in 2008 from the Department of Computer Science at the University of Toronto under the supervision of Stephen Cook.\nIn 2007, together with Alex Hertel and Philipp Hertel, I co-founded Zetawire. It was acquired by Google from stealth mode in 2010, and formed the base for the Google Wallet product.\nBetween 2008 and 2010 I spent two years as a postdoctoral researcher at the Microsoft Research New England lab.\nIn 2010-11 I was an assistant professor jointly appointed at the Departments of Mathematics and Computer Science at the University of Toronto.\nI have been a professor of computer science at Princeton since 2015. I joined Princeton in 2011 as an assistant professor.\nWith my students and postdocs I work in complexity theory, the theory of real computation, machine learning, algorithms, game theory, and applications of computer science in healthcare and medicine. You can find my publications here. My research is supported by a number of awards, including a 2011 Sloan Fellowship and a 2013 Packard Fellowship.\n"},{"id":3,"href":"/team/members/","title":"Members","parent":"Group","content":"Current Group Members Graduate students:  Antonio Molina Lovett  Postdocs:  Vijay Bhattiprolu John Peebles Sahil Singla  Alumni: With first place of employment/study:\nGraduate students  Sumegha Garg PhD in Computer Science, Princeton, 2020; Rabin Postdoctoral Fellow at Harvard. Young Kun Ko PhD in Computer Science, Princeton, 2018; Assistant Professor / Faculty Fellow at NYU. Jieming Mao PhD in Computer Science, Princeton, 2018; Google Research New York. Jonathan Schneider, PhD in Computer Science, Princeton, 2018; Google Research New York. Ankit Garg, PhD in Computer Science, Princeton, 2016; Microsoft Research New England. Omri Weinstein, PhD in Computer Science, Princeton, 2015; NYU postdoc supported by the Simons Society of Fellows. Anastasios Zouzias, PhD in Computer Science, Toronto, 2013; IBM Research, Zurich. Wei Xi Fan, MSc in Mathematics, Toronto, 2011; Google Canada. Rinoc Johnson, MSc in Mathematics, Toronto, 2011; MapleSoft.  Postdocs I supervised or worked with closely  Shay Moran, 2017-2020, IAS, Princeton, and Google; Faculty at the Technion, Israel. Dor Mintzer, 2018-2020, IAS; Faculty at MIT. Sepehr Assadi, 2018-2019, Princeton; Faculty at Rutgers University. Gil Cohen, 2016-18, Princeton; Faculty at Tel-Aviv University, Israel. Ran Gelles, 2014-16, Princeton; Faculty at Bar-Ilan University, Israel. Matt Weinberg, 2014-16, Princeton; Faculty at Princeton University. Rotem Oshman, 2013-14, Princeton; Faculty at Tel-Aviv University, Israel. Klim Efremenko, 2012-13, IAS; Postdoc at the University of Chicago. Ankur Moitra, 2011-13, IAS; Faculty at MIT. Jing Chen, 2012-13, IAS; Faculty in Stony Brook University. Eden Chlamtac, 2010, Toronto; Faculty in Ben Gurion University, Be\u0026rsquo;er Sheva, Israel. Per Austrin, 2010-11, Toronto; Faculty in KTH, Stockholm, Sweden. Cristobal Rojas, 2010-11, Toronto; Faculty in Universidad Andres Bello, Santiago, Chile.  Undergraduate students See this list of past advised projects.\n"},{"id":4,"href":"/about/affiliations/","title":"Affiliations and service","parent":"About","content":" Princeton Computer Science Theory Group PACM (Program in Applied and Computational Mathematics) Simons Institute for the Theory of Computing Scientific Advisory Board Foundations of Computational Mathematics Board of Directors  "},{"id":5,"href":"/research/current/","title":"Current","parent":"Research","content":"This page is under construction. It will cover current/near-future research direction (mainly for prospective students).\nCurrent and near-future research directions Mechanism design, algorithms, and learning Computational complexity theory Algorithms and lower bounds for new models of computation Later I will add distant-future research directions.\n"},{"id":6,"href":"/research/dynamics-and-computation/","title":"Dynamics and computation","parent":"Research","content":"A dynamical system is a stateful system (often with a continuous state space) evolving over time. Thus, dynamical systems can be (and are being) used to capture the behavior of both natural and articifical systems over time. The state space of dynamical systems is typically continuous, which means that one has to look at continuous models of computation. Here is a short 2013 Communications of the ACM article I wrote explaining computation over the reals. There is also an older 2006 article with Stephen Cook in the Notices of the AMS.\nThere are at least two natural connections between dynamics and computation. In one direction, the question \u0026ldquo;which properties of which dynamical systems can be computed and how efficiently?\u0026quot; is foundational to mapping the limits of applied mathematics. In the opposite direction, \u0026ldquo;how powerful a computation can a given dynamical system simulate robustly?\u0026quot; touches upon the Church-Turing thesis and questions of hypercomputation.\n Which properties of dynamical systems can be computed? The case of Julia sets Dynamical systems as computational devices and the Space-Bounded Church-Turing Thesis  Which properties of dynamical systems can be computed? The case of Julia sets Julia sets are some of the most visualized objects in Mathematics. They arise in the study of complex dynamics (that is, the state space if the set $\\mathbb{C}$ of complex numbers, visualized on the plane), along with the Mandelbrot set. From the scientific perspective, the family of Julia sets and the complex dynamics that gives rise to them, are sufficiently rich to exhibit phenomena one sees in the \u0026ldquo;wild\u0026rdquo;. At the same time, after 100+ years of deep study, we know a lot about them, making them good \u0026ldquo;lab models\u0026rdquo; for more general dynamical systems. Computer programs had been written to visualize Julia sets (by amateur programmers and professional mathematicians alike). Thus questions about computational properties of Julia sets are natural to consider, and may shed light on the broader set of phenomena we should expect in \u0026ldquo;computablility of dynamical systems\u0026rdquo;.\nConsider the quadratic function $f_c:\\mathbb{C}\\rightarrow \\mathbb{C}$ given by $f_c(z):=z^2+c$. For any initial point $z_0$, the mapping $f_c$ induces a (discrete-time) trajectory of the evolution of $z$ under $f_c$: $z_1=f_c(z_0)=z_0^2+c$, $z_2=f_c(z_1)=(z_0^2+c)^2+c$, and more generally $z_{t+1}=f_c(z_t)$.\nThis evolution, viewed as a dynamical system, raises quesitons of the form \u0026ldquo;what will happen to the trajectory eventually?\u0026quot; and \u0026ldquo;what can be said about the set of trajectories as a whole?\u0026quot;.\nIf $|z_0|$ is sufficiently large, e.g. $|z_0|\u0026gt;|c|+1$, then we will have $|z_1|\u0026gt;|z_0|$, and the trajectory will rapidly escape to $\\infty$. On the other hand, if $z_0$ is a root of the polynomial $z^2+c=z$, then it will be a fixed point of the dynamical system, and its trajectory will not escape to $\\infty$. The filled Julia set in this case is the set of initial conditions for which the dynamics does not escape to $\\infty$: $$ K_c:=\\lbrace z_0: z_t\\nrightarrow\\infty\\rbrace. $$ The Julia set is the boundary of $K_c$: $J_c:=\\partial K_c$. It is the set of points around which the long-term behavior is unstable: if $z\\in J_c$, then any neighborhood of $z$ contains both points whose trajectories escape to $\\infty$ and those whose trajectories stay bounded.\nAs it turns out, there exist non-computable quadratic Julia sets:\nTheorem [BY'06]: There exists a parameter $c$ such that no Turing Machine with access to parameter $c$ can compute arbitrarily precise picture of $J_c$.\nMoreover, it turns out that such a $c$ can be computed explicitly [BR07].\nWhen a non-computability result is present, one might expect non-computability to be the rule, and computational tractability to be a lucky exception. Quadratic Julia sets are an interesting case-study, since they are rich enough to support many dynamically interesting behaviors, but are sufficiently well-understood to allow us to answer most questions about their computational properties. Surprisingly, non-computability sometimes occurs, but it is the exception.\nIn fact, by several measures, we either know that \u0026ldquo;most\u0026rdquo; quadratic Julia sets are computable, or strongly suspect that they are. Intuitively, the non-computable examples are constructed by carefully encoding a computationally difficult function into the parameter $c$ at infinitely many scales (more precisely, the encoding uses a continued fraction expansion). Perturbing $c$ by even a tiny amount will destroy most of this encoding.\nDynamical systems as computational devices and the Space-Bounded Church-Turing Thesis Returning to the broader question of \u0026ldquo;which properties of dynamical systems can be computed?\u0026rdquo;, one can loosely calssify computational hardness results into two categories: \u0026ldquo;systems that can simulate generic computation\u0026rdquo; and \u0026ldquo;problems into which hard-to-compute functions can be carefully woven\u0026rdquo;.\nIf a dynamical system is rich enough to simulate a Turing Machine (and thus generic computation), then diagonalization results starting with the intractability of the Halting Problem and the time hierarchy theorems imply that the only way to answer questions about their long-term behavior is through simulation. Cellular automata generally belong to this category.\nEven if a dynamical system is not rich enough to simulate generic computation, one can still carefully reduce some non-computable function to some long-term property of the dynamical system. It is very unlikely that all computation can be encoded into iterations of a fixed quadratic polynomial over $\\mathbb{C}$), but we can carefully construct a $c$ such that computing arbitrarily high precision images of $J_c$ is as hard as solving the Halting Problem.\nA closely related question is that of robustness. If we perturb the dynamical system a little bit, will the non-computability phenomenon disappear? One would expect the answer to be \u0026lsquo;yes\u0026rsquo; in the second case, and \u0026lsquo;depends\u0026rsquo; in the first case. If the non-computable example had to be carefully constructed, then noise is likely to destroy it (as it does in the case of quadratic Julia sets). More generally, what can be said about the computational complexity of noisy dynamical systems?\nIn full generality, this question is too philosophical to be precisely formulated as a purely mathematical conjecture. It is similar in flavor to the Church-Turing Thesis that asserts that all physically relevant dynamical systems are at most as powerful as a Turing Machine (or any other general-purpose computing device) for the purposes of computability. The best-known quantitative refinement of the CT Thesis, sometimes called the Extended Church-Turing Thesis (probably falsly \u0026ndash; see quantum computing) extends the connection to time complexity: any computation by a dynamical system can be efficiently simulated in time polynomial in the size of the system and its original runtime. Relatively little attention has been given to space (that is memory) complexity in this context, even though memory complexity is a more robust notion, and is easier to interpret than time in the context of dynamical systems.\nThe reason that properties of dynamical systems under noise become computationally tractable is that in the presense of noise these systems are only able to maintain a limited amount of memory over time. A system capable of robustly representing only $2^M$ distinct states (corresponding to $M$ bits of memory) can be analyzed by a Turing Machine using only $poly(M)$ memory (though potentially in time exponential in $M$). How one might define the amount of \u0026ldquo;memory\u0026rdquo; a system has? Information theory comes in handy here.\nSuppose the evolution of the system is given by the sequence $X_1,X_2,\\ldots$, where $X_{t+1}$ is obtained from $X_t$ as $$X_{t+1}=N_\\varepsilon(\\Phi(X_t)),$$ where $\\Phi(X_t)$ is the (noiseless) evolution rule, and $N_\\varepsilon$ is a noise operator. We can define the memory of the system as: $$ M:= I(X_{t+1};X_t), $$ the amount of information the next step retains about the previous one.\nThe system\u0026rsquo;s dimension plays a critical role in the amount of memory it retains. If the state space of $X_t$ is bounded and has $d$ dimensions, then under random noise of magnitude $\\varepsilon$ we should expect the memory to be $M\\sim d\\cdot \\log (1/\\varepsilon)$: memory scales liniarly in dimension but only logarithmically in the noise.\nWith a \u0026ldquo;definition\u0026rdquo; of memory at hand, we can postulated a space complexity version of the Church-Turing thesis. We call it the Space-Bounded Church-Turing thesis (SBCT):\nSpace-Bounded Church-Turing thesis [BRS'15]: A dynamical system with memory capacity $M$ is at most as powerful as a Turing Machine with $poly(M)$ bits of memory.\nUnlike the extended-CT (which talks about time complexity), SBCT does not appear to contradict our current understanding of the limits of quantum computing. A stronger (and mathematically more formalizable) assertion is that whenever the noise itself is computationally simple (and thus is not a source of more computational complexity), long-term properties of a system with $M$ bits of memory can be computed by a Turing Machine with $poly(M)$ space. This indeed can be shown in some interesting special cases [BRS'17].\nThe SBCT is consistent with the fact that non-computability in the context of Julia sets is not robust to noise. The state space over which the dynamics defining the Julia sets operates is $\\mathbb{C}$, with error $\\varepsilon$ such a system only has $\\sim\\log (1/\\varepsilon)$ memory. On the other hand, the memory of a cellular automaton (even with noise) scales with the size of its board (each cell can \u0026ldquo;remember\u0026rdquo; $\\sim 1$ bit of information), which is typically infinite. Therefore, their long-term properties are potentially undecidable even in the presense of noise, again, consistently with SBCT.\nFurther reading Surveys on computing over the reals:\n Notices of the AMS, 2006 Communications of the ACM, 2013  On Julia sets:\nBook: Computability of Julia Sets\nMark Braverman, Michael Yampolsky\nSpringer, 2009\n[Amazon] [Springer]\nOn computability in dynamics more generally and the Space-Bounded Church-Turing thesis:\n A 2012 talk I gave at the 2012 Turing Centenary conference in Manchester. A phys.org article about the SBCT.  "},{"id":7,"href":"/research/information-complexity/","title":"Information complexity","parent":"Research","content":"Information theory is a vastly successful theory underpinning much of our communication technology. In many important communication scenarios it allows one to calculate the precise amount of resources needed to perform a cetrain task. For example, the number of bits Alice needs to communicate to send a string $x_1 x_2 \\ldots x_n$ of $n$ random trits (elements from $\\lbrace 1,2,3\\rbrace$) to Bob is given by $(\\log_2 3) \\cdot n \\pm o(n)$. The number of bits Alice needs to communicate to send $n$ random trits to Bob if Bob already knows half of the $x$\u0026rsquo;s (regarless of whether Alice knows which locations are known to Bob) is $\\frac{\\log_2 3}{2}\\cdot n \\pm o(n)$ etc. These quantities can be written as expressions in terms of things like Shannon\u0026rsquo;s entorpy, mutual information, and conditional mutual information.\nNotions of entropy and mutual information allow to precisely (and losslessly!) describe the flow of information in a variety of settings. They also allow to one to write \u0026ldquo;common sense\u0026rdquo; statements about information in precise mathematical terms which can then be manipulated. This turns out to be extremely useful in reasoning about communication. For example entropy $H(M)$ represents the amount of uncertainty in a message $M$. Mutual information $I(M;X)$ represents the amount of information the message $M$ reveals about a piece of data $X$. Conditional mutual information $I(M; X|Y)$ represents the amount of information a message $M$ reveals about $X$ to someone who already knows a piece of data $Y$.\nThe chain rule $$ I(M_1 M_2; X) = I(M_1; X) + I(M_2; X|M_1) $$ is just a precise way to formalize the intuitive fact that what one learned from two messages $M_1 M_2$ about $X$ can be decomposed into what one learned from the first message plus what one learned from the second message when we already knew the first one. The data processing inequality $$ I(X;F(Y)) \\le I(X;Y) $$ formalizes the intuition that a function computed on $Y$ cannot reveal more about $X$ than $Y$ itself.\nThe goal of information complexity is to learn to apply information-theoretic formalism to computational settings. As of now (2021), these tend to work beautifully in models of computation that are simple enough to not support information-theoretically secure computation. Understanding why exactly this happens is an interesting direction for future work. Below we give some examples of results based on information complexity from the 2010-2020 period. In some cases the results are new, while in others they give conceptually simpler proofs or more general statements of existing theorems.\nTwo party communication In the two-party communication complexity setting, two players (Alice and Bob) are given inputs $X$ and $Y$. They are also allowed to use a randomness source $R$. A protocol $\\pi$ is just a formalization of a conversation: each message in $\\pi$ is allowed to depend on the speaker\u0026rsquo;s input, on the conversation so far, and on the public randomness. The communication cost of a protocol is the number of bits communicated during its execution. The communication complexity $CC(T)$ of a task $T$ is the smallest communication cost of a protocol $\\pi$ solving $T$. In the context of communication complexity, $T$ is typically the task of \u0026ldquo;computing a given function $F(X,Y)$ with error probability $\u0026lt;\\varepsilon$\u0026rdquo;.\nAn instructive example is the Equality problem. Alice is given an $n$-bit string $X$, Bob is given an $n$-bit string $Y$, and they would like to determine whether $X=Y$. It can be shown that this can be accomplished with error $\u0026lt;\\varepsilon$ using $k \\sim \\log (1/\\varepsilon)$ bits of communication. Alice will compute a random hash $h(X)$ of lenght $k$ on her input $X$ and send the value to Bob. Bob will compare $h(X)$ to $h(Y)$, and will return \u0026lsquo;equal\u0026rsquo; if they match. There is a $2^{-k}$ probability of hash collision, which means that \u0026lsquo;equal\u0026rsquo; is returned with probability $\\approx 2^{-k}$ even when $X\\neq Y$. Interestingly, a zero-error protocol for equality requires $n+1$ bits of communication.\nAnother function with many applications in lower bounds is Disjointness. Alice and Bob are given subsets $X,Y\\subset \\lbrace 1,\\ldots,n\\rbrace$ (wich can be represented as length-$n$ bit-strings), and need to determine whether they have an element in common. $Disj_n(X,Y)=1$ if the sets are disjoint, and $0$ if they intersect.\nDirect sum and information complexity The direct sum problem (in any model of computation) asks whether it costs $k$ times as much to perform $k$ independent copies of a task $T$ as it costs to perform one copy, or whether one gets a \u0026ldquo;volume discount\u0026rdquo;. When direct sum holds, one gets a powerful lower bounds tool \u0026mdash; a lower bound $L$ on $T$ gets amplified into a lower bound of $k\\cdot L$ on the task $T^k$ of performing $k$ copies of $T$. When direct sum fails, new interesting algorithms follow. A famous example of such a failure is matrix-vector multiplication. A counting argument shows that one needs $\u0026gt;n^2$ operations to mutiply an $n\\times n$ matrix $A$ by a vector $v$. At the same time, multiplying $A$ by $n$ different vectors $v_1,\\ldots,v_n$ is just the problem of multiplying two $n\\times n$ matrices, which can be done faster than $n\\cdot n^2=n^3$ via fast matrix multiplication.\nThe Direct Sum Problem for randomized communication complexity asked whether the direct sum property holds for randomized communication complexity. That is, whether $CC(T^k)\\gtrsim k\\cdot CC(T)$? It turns out that trying to answer this question is closely related to understanding the information complexity of $F$.\nInformation complexity is defined similarly to communication complexity, with information cost replacing communication cost. The (two-party) information cost of a protocol $\\Pi$ on inputs $(X,Y)$ is defined to be the amount of information the participants learn about each other\u0026rsquo;s inputs during the execution of the protocol. Fortunately, the information-theoretic notation makes this quantity easy to formalize: $$ IC(\\Pi):= I(X;\\Pi|Y) + I(Y;\\Pi|X). $$ The first term corresponds to what Bob (who knows $Y$) learns about Alice\u0026rsquo;s input ($X$) from the protocol $\\Pi$. The second term corresponds to what Alice learns. The information complexity of a task $T$ over inputs $(X,Y)$ is defined as the smallest possible information cost of a protocol solving $T$. $$ IC(T):= \\inf_{\\text{$\\Pi$ solves $T$}} IC(\\Pi). $$ The infimum here is necessary, since it is possible that there is a sequence of successful protocols $\\Pi_1,\\Pi_2,\\ldots$ that get ever longer while revealing an ever smaller amount of information (in fact, this happens for the simple task of computing the two-bit AND function).\nIt turns out that the amortized (per-copy) communication complexity of a task $T$ is equal to its information complexity, at least when a vanishing amount of error is allowed. Let $T(X,Y)$ be a task that allows for a small amount of error $\\ve=o(1)$. Let $CC(T^k)$ be the communication complexity of $k$ copies of $T$, and let $IC(T)$ be its information complexity. Then information is equal to amortized communication:\nTheorem [BR'11]: $\\displaystyle{\\lim_{k\\rightarrow\\infty} \\frac{CC(T^k)}{k} = IC(T)}$.\nInteractive compression This theorem gives an immediate blueprint for proving direct sum theorems for (randomized) communication complexity. To show (for example) that $CC(T^k) \\gtrsim k\\cdot CC(T^k)$, one can show that $IC(T)\\gtrsim CC(T)$. This inequality, more commonly written as $CC(T)\\lesssim IC(T)$ is known as the interactive compression question. It asks whether a protocol $\\pi$ that solves $T$ while revealing little information can be \u0026ldquo;compressed\u0026rdquo; into a protocol $\\pi'$ that uses little communication.\nIn the context of one-way communication near-perfect compression is generally possibly. For example Huffman coding allows one to encode a message with entropy $H(M)$ using at most $H(M)+1$ bits in expectation. It is generally the case that a protocol with $r$ rounds of communication can be compressed into $O(IC(\\Pi)+r)$ bits of communication. Unfortunately, the protocol achieving $IC(T)$ may have an unbounded number of rounds, making such a compression useless in the general interactive setting.\nIt turns out that it is possible to compress a general protocol $\\pi$ whose information cost is $I$ and whose communication cost is $C$ into a protocol $\\pi'$ whose communication cost is $\\tilde{O}(\\sqrt{I\\cdot C})$. This leads to a partial direct sum theorem for randomized communication:\nTheorem [BBCR'10]: $\\displaystyle{CC(T^k)}=\\tilde{\\Omega}(\\sqrt{k}\\cdot CC(T))$.\nAt the same time, Ganor, Kol, and Raz showed a separation between information and communication complexity:\nTheorem [GKR'15]: There is a family of functions whose information complexity is $n$ and whose communication complexity is $2^{\\Omega(n)}$.\nMoreover, it can be shown that this separation is tight - communication complexity is always at most exponential in information complexity. The GKR'15 theorem rules out a tight direct sum theorem for communication complexity. It still remains open whether the $\\sqrt{k}$ in BBCR'10 can be improved, e.g. to $k^{1-\\varepsilon}$.\nDirect product and parallel repetition: using information formalism In light of the BR'11 and GKR'15 results, information complexity becomes the \u0026ldquo;correct\u0026rdquo; measure for studying amortized cost of two-player randomized communication complexity. The connection from BR'11 between information and amortized communication can be further deepened by a direct product theorem. A direct sum theorem gives a lower bound on the amount of resources required to accomplish $k$ copies of a task. A direct product theorem further says that trying to solve $k$ copies of the task with fewere resources will fail except with an exponentially small probability.\nTheorem [BRWY'13,BW'14]: If $I$ bits of information are required to compute a single copy of a function $f(x,y)$ under an input distribution $\\mu$, then any communication protocol that attempts to solve $k$ copies of $f$ with input distributed according to $\\mu^k$ using $o(k\\cdot I)$ bits of communication will fail except with an exponentially small probabiliy $2^{-\\Omega(k)}$.\nAt the heart of the proof of the direct product theorem is applying information-theoretic formalism which treats the \u0026ldquo;success event\u0026rdquo; as just another piece of information. Formalism such as the chain rule allows to make statements of the form \u0026ldquo;succeeding on the first copy does not reveal too much about inputs to the second copy\u0026rdquo; formal.\nPreviously, such reasoning has been used successfully to prove the parallel repetition theorem for two-prover games. Two-prover games warrant a separate discussion, but it should be mentioned that formalism from the direct product theorem from communication complexity can be lifted to re-prove the parallel repetition theorem, and give the only known proof of the parallel repetition theorem in the low-success-probability regime [BG'15].\nThe two-bit AND function and Disjointness Turning from abstract complexity-theoretic results to concrete ones, let us consider the information complexity of specific functions. The simplest functions are ones are of the form $f:\\lbrace 0,1\\rbrace\\times\\lbrace 0,1\\rbrace \\rightarrow \\lbrace 0,1\\rbrace$, where Alice and Bob are each given a single bit of input. Of those, only the AND function (and equivalent transformations) is interesting from the information complexity perspective. Other functions are either constant, amount to one-way data transmission (projection functions), or to a two-way data-exchange (the XOR function).\nGenerally speaking, we do not know an efficient procedure for computing the information complexity of a function from its truth table. In fact, even proving that information complexity is computable appears to be non-trivial [BS'16]. Fortunately, in the case of the two-bit AND, it is possible to describe the information-theoretically optimal protocol.\nGenerally speaking, the optimal protocol depends not only on the function $f$ being computed but also on the prior distribution $\\mu$ of inputs - this is because the amount of information revealed by a protocol $\\pi$ depends on the prior distribution $\\mu$.\nLet $f(x,y)=x\\wedge y$ be the two-bit AND function, and assume, for simplicity, that the prior distribution $\\mu$ is symmetric: $\\mu(0,1)=\\mu(1,0)$.\nThe ascending clock protocol [BGPW'13]:\n If $x=1$, Alice sets $A:=1$, otherwise picks a uniformly random $A\\in_U [0,1]$; If $y=1$, Bob sets $B:=1$, otherwise picks a uniformly random $B\\in_U [0,1]$; A continuous clock counts time $t$ from $0$ to $1$; If $A$ is reached, Alice raises her hand and the protocol terminates; If $B$ is reached, Bob raises his hand and the protocol terminates; If the protocol terminates at time $t\u0026lt;1$ output $AND(x,y)=0$; If the protocol terminates at time $t=1$ output $AND(x,y)=1$.  Note that the ascending clock \u0026ldquo;protocol\u0026rdquo; is not really a protocol, since it runs in continuous time. It can be approximated by an $r$-round protocol $\\pi_r$ with each step representing the clock running for $\\frac{1}{r}$ time units. The information cost of such $\\pi_r$ is $\\sim \\frac{1}{r^2}$ bits higher than optimal, which is only attained at the limit. Thus even for the two-bit AND function, optimal information complexity is only attained in the limit!\nBased on the optimality of the ascending clock protocol, one can get exact bounds on several two-party communication problems that are based on $n$ copies of the two-bit AND function:\n Intersection. The randomized communication complexity of finding the intersection of two subsets of $\\lbrace 1,\\ldots,n\\rbrace$ (which amounts to $n$ copies of AND) is $C_\\wedge \\cdot n \\pm o(n)$, where $C_\\wedge\\approx 1.4923$ is a constant obtained by maximizing an explicit function.  Interestingly, when no error is allowed, the constant increases to $\\log_2 3\\approx 1.585$ [AC'94]. 2. Disjointness. The randomized communication complexity of deciding whether two subsets of $\\lbrace 1,\\ldots,n\\rbrace$ intersect (which amounts to $n$ copies of AND where the prior $\\mu$ puts zero weight on $(1,1)$) is $C_{DISJ} \\cdot n \\pm o(n)$, where $C_{DISJ}\\approx 0.4827$. 3. **Small set Disjointness** The randomized communication complexity of deciding whether two subsets of $\\lbrace 1,\\ldots,n\\rbrace$ of size $k$ intersect (which amounts to $n$ copies of AND where the prior $\\mu$ puts zero weight on $(1,1)$ and $\\frac{k}{n}$ weight on the $(0,1)$ and $(1,0$ entries) is $\\frac{2}{\\ln 2} \\cdot k \\pm o(k)$.\nIn this case, even the fact that the communication complexity is $O(k)$ and not $O(k \\log k)$ is somewhat surprising [HW'07].\nDoes one need memory to approximately count? As discussed earlier, the strength of information-theoretic formalism is its ability to formalize vague statements about states of \u0026ldquo;knowledge\u0026rdquo; into precise (and correct) mathematical statements. Within complexity theory there are many examples of this from the last two decades. Let us highlight an easy-to-state recent example, motivated by streaming algorithms.\nAn algorithm is given access to a sequence $x_1,\\ldots,x_n$ of uniformly random bits. At the end, it needs to guess whether there were more $0$\u0026rsquo;s in the sequence or more $1$\u0026rsquo;s. The algorithm only needs to be correct on $51%$ of the inputs. This means that it is ok to only ouput $1$ on sequences with more than $\\frac{n}{2}+\\sqrt{n}$ $1$\u0026rsquo;s, output $0$ on sequences with more than $\\frac{n}{2}+\\sqrt{n}$ $0$\u0026rsquo;s, and give a random answer otherwise. The constrained resource is the amount of memory $S$ the algorithm can store while reading the stream.\nOne obvious solution is to maintain a register that at time $t$ stores $R_t=\\sum_{j=1}^t x_t$, and outputing $1$ if $R_n\u0026gt;n/2$. This solution requires memory $S=\\log n + O(1)$. Can one do substantially better? It turns out that the answer is tight:\nTheorem [BGW'20] Solving the approximate majority problem requires memory $S\\ge \\Omega(\\log n)$.\nNote that even though the theorem\u0026rsquo;s statement appears \u0026ldquo;obviously true\u0026rdquo;, it is more delicate than one would expect: (1) it is possible to distinguish sequences with $\\frac{n}{2}+n^{0.67}$ $1$\u0026rsquo;s from uniform sequences using only $O(\\log \\log n)$ memory; (2) it is possible to solve the problem using $O(\\log n)$ memory while only storing $O(1)$ information about the $x_j$\u0026rsquo;s seen so far. The relevant quantity which must be at least $\\Omega(\\log n)$ for a typical $t$ turns out to be:\n$$ \\sum_{j=1}^t I(R_t; X_j | R_{j-1}). $$\nOnce this quantity is correctly identified, the proof follows relatively standard proof patterns.\nBeyond two party communication? It is very interesting to consider the kinds of problems where information-theoretic reasoning has not been successful so far. It is natural to try the same approach for more than two parties. In a three-party setting one can define the information complexity of problems based on the amount of information the participants need to reveal to each other in order to solve the problem. It is not difficult to prove a direct sum theorem showing that the (3-party) information complexity of $k$ copies of a task $T$ are $k$ times the information complexity of a single copy. Unfortunately, such a statement turns out to be vacuous! Communication amoung 3 or more \u0026ldquo;honest-but-curious\u0026rdquo; parties supports information-theoretically secure multi-party computation, which means that any function can be computed by 3 or more parties without revealing anything to each other (except for the answer in the end). Thus the \u0026ldquo;direct sum\u0026rdquo; theorem in this case turns out to be of the form \u0026ldquo;$k\\times 0 = 0$\u0026rdquo;.\nThis pattern appears to repeat itself in any computational model that is rich enough to support information-theoretically secure computation. For example, consider communication over a channel where Alice and Bob each submit a bit $x_i$ and $y_i$, and learn the value $x_i\\wedge y_i$ of the AND of the two bits. Information-theoretic lower bounds over such a channel would have had interesting complexity-theoretic implications. Once again, one can redo information complexity over such a channel and obtain direct sum results. Once again, the result is vacuous, since such a channel is powerful enough to be able to implement information-theoretically secure two party computation [K'91]. The same is true in the context of Arthur-Merlin games [GPW'16].\nUnderstanding whether this is a surmountable technical barrier, or a true conceptual one (that requires new tools or even new conjectures) is a topic which I hope to work on in the future.\nFurther reading  A 15 minutes high level overview on theoretical computer science and information complexity: [pdf] A survey can be found here. Quantum information complexity is a very interesting topic not covered above. Some pointers can be found here and here. The papers section on this page.  "},{"id":8,"href":"/team/joining-the-group/","title":"Joining the group","parent":"Group","content":"Thank you for you interest in joining the group!\nUnfortunately, due to large volumes of email, I am not able to respond to individual inquiries about joining Princeton or our group. Some general details can be found below.\nHigh school and undergraduate students   If you are a Princeton undergrad, and are interested in the possibility of doing Junior/Senior independent work, please email me to make an appointment and discuss your interests. You can browse this page and look at the papers to get an idea of the topics we\u0026rsquo;re working on. Please also look at potential IW topics here.\n  I do not have internships for undergrads from outside Princeton. That said, truly exceptional undergrads (e.g. IMO/IOI medalists, top performance in Putnam, ACM programming world finalists, etc.) are welcome to contact me for potential opportunities.\n  Unfortunately, I do not have research opportunities for high school students.\n  If you have been admitted to the undergraduate program at Princeton, and are in the process of choosing between colleges and would like to meet, please send me an email. If you are considering or in the process of applying, I would prefer to wait until the admissions process runs its course before meeting. Note that I have no influence on the admissions decisions.\n  Prospective graduate students   We are recruiting outstanding graduate students!\n  If you are a current Princeton graduate student and would like to talk about the possibility of working with me, write me an email.\n  If you are not currently a graduate student at Princeton, you have to apply for admission first. More information can be found here. Please follow the procedure for applying. Please note that the selection of PhD students to be admitted is a competitive process based on the merits of individual applications.\n  You may also be interested in our funded Master\u0026rsquo;s program.\n  Emailing me does not improve your chances of being admitted. If anything, \u0026ldquo;mass\u0026rdquo; emails might slightly hurt your chances. If you are interested in working with me, please apply to the Theoretical Computer Science track and mention that you would like to work with me in your statement, and I will look at your file during the admissions process.\n  I cannot (nor am willing to try to) estimate your chances of being admitted to a graduate program in Princeton CS. The admissions process is complicated, and looks at a variety of factors in the application, including grades, personal statement, and recommendation letters.\n  If you feel that due to unusual circumstances your application is at a high risk for being overlooked, please have your academic adviser or senior program administrator send me a personal email.\n  Postdocs   We usually have a number of postdoc opportunities through the Theory Group, please see here.\n  If you feel that due to unusual circumstances your application is at a high risk for being overlooked, please have your PhD adviser send me a personal email.\n  "},{"id":9,"href":"/research/mech-design/","title":"Mechanism design","parent":"Research","content":" Black-box reductions from algorithms to mechanisms without money Strategic capitation: mechanism design in the \u0026ldquo;big data\u0026rdquo; regime Mechanism design for learning agents  Black-box reductions from algorithms to mechanisms without money Further reading:\n Preprint: arXiv  Strategic capitation: mechanism design in the \u0026ldquo;big data\u0026rdquo; regime Mechanism design for learning agents "},{"id":10,"href":"/research/past-projects/","title":"Misc projects","parent":"Research","content":"Contents  New bounds on the Grothendieck constant Monotonicity and implementability  New bounds on the Grothendieck constant Let $A$ be an $n\\times n$ matrix. It gives rise to the quadratic form $$\\sum_{i,j} A_{ij} x_i y_j.$$ The Grothendieck constant $k$ bounds the ratio between the maximum of this form when $x_i,y_j$ are unit vectors in a Hilbert space and the maximum of this form when $x_i,y_j\\in {-1,1}$ are just real numbers. In other words, $k$ is the smallest number such that for all $A$,\n$$\\left|\\max_{||X_i||,||Y_j||\\leq 1} A_{ij} \\langle X_i, Y_j \\rangle\\right| \\leq k\\cdot \\left|\\max_{x_i,y_j\\in {-1,1}} A_{ij} x_i y_j\\right|.$$\nAlgorithmically, the Grothendieck constant can be viewed as the integrality gap between what can be attained by a general (vector) solution and the integral solution to the optimization problem of maximizing $\\sum_{i,j} A_{ij} x_i y_j$. A natural strategy for giving an upper bound on $k$ is to come up with a (possibly randomized) _rounding scheme_ that converts vectors $X_i, Y_j$ into numbers $x_i,y_j$ while losing a factor of at most $k$ in the objective function.\nRounding vectors to numbers has had many important applications in algorithms. For example, the Goemans-Williamson MAX CUT approximation algorithm represents the MAXCUT instance as a (tractable) optimization problem over vectors, and then uses a random projection onto a line to obtain an approximate integral solution. The approximation ratio is exactly the worst-case ratio between the value attained by the vector solution versus the integral solution.\nIn 1977, Krivine proposed a natural rounding scheme for the Grothendieck inequality, proving an upper bound $k\\le c_k= \\frac{\\pi}{2\\ln (1+\\sqrt{2})}\\approx 1.7822$, which he conjectured to be tight. The scheme (similarly to the rounding schemes in the algorithms from decades to follow), involved transforming the vectors and then applying a random projection of the vectors to a line (with vectors projected to the positive side assigned a $+1$, and those to the negative side a $-1$). In 2011 we disproved Krivine\u0026rsquo;s conjecture that $k=c_k$ by showing that $k\u0026lt;c_k$.\nThis is done by devising a better rounding scheme (or, rather showing that one exists via a delicate perturbation argument). In the process, we show that, most likely, a two-dimensional rounding strategy can beat the one dimensional strategy of projecting vectors onto a line. The two-dimensional strategy would partition the plane into two regions $R_{+1}$ and $R_{-1}$, and map vectors to numbers by projecting them to the plane and then assigning them to $\\pm 1$ based on the region into which they fall. The one-dimensional strategy corresponds to $R_{+1}$ and $R_{-1}$ each being a half-plane.\nThe diagram on the right gives an educated guess on what the partition of the plane $\\mathbb R^2$ might look like for the optimal two-dimensional Krivine scheme. If Krivine\u0026rsquo;s conjecture were true, the optimal partition would have been just a half-plane.\nReference: Mark Braverman, Konstantin Makarychev, Yury Makarychev, Assaf Naor. The Grothendieck constant is strictly smaller than Krivine\u0026rsquo;s bound, 2011.\nLinks: paper on arXiv; FOCS'11 version from IEEE; open access version from Forum of Mathematics, Pi.\nMonotonicity and implementability Consider an environment with a finite set of alternatives $A_1,\\ldots,A_k$ and agents who have quasi-linear preferences. That is, each agent $i$ has utility $u_{ij}$ for alternative $A_j$, and if she has to pay an amount $p_i$ her utility becomes $u_{ij}-p_i$. A direct mechanism consists of an allocation rule $f$ and payment rule $p$. The allocation rule maps each profile of valuations to a probability vector over the set of possible alternatives. For example, the second price auction on one item amounts to the allocation rule being “the highest bidder gets the item”, and the payment collected from the winner equals the second-highest bid (and no payment collected from other players).\nAn allocation rule $f$ is said to be implementable if there exists a payment rule $p$ so it is safe for each agent to report his true valuation to the mechanism $(f,p)$\u0026quot;) regardless of the reports of all other agents. In other words, the payment rule $p$ makes truthful reporting a dominant strategy for all players. Understanding which allocation rules are implementable is a fundamental concern in mechanism design. Myerson has shown in his seminal 1981 paper, that when the set of alternatives is single dimensional (for example when a single item is auctioned for sale), an allocation rule is implementable if an only if it is monotone in the valuation of a player. Informally, monotonicity means that higher demand for an outcome by a player will result in a greater-or-equal allocation of that outcome.\nFor multidimensional settings, such as selling multiple goods, Rochet has shown in 1987 that a stronger condition, called cycle-monotonicity, of an allocation rule is a necessary and sufficient condition for implementing it. Cyclic monotonicity however is a considerably more difficult condition to work with than monotonicity. Monotonicity is a condition on every pair of values, whereas cyclic monotonicity is a condition on every finite sequence of values. This gives rise to the natural question of when does monotonicity imply cycle-monotonicity (and thus implementability).\nSaks and Yu (2005) have shown that when the domain of (multidimensional) valuations is convex, monotonicity is necessary and sufficient for implementing an allocation rule. We show that convexity is also necessary in the following sense: if the domain of valuation is not convex, there always exists an allocation rule that is monotone yet not implementable. In other words, when the domain of possible valuations is not convex, local constraints alone are not sufficient to guarantee implementability.\nFor each non-convex domain, the proof involves the construction of an allocation rule that is not implementable. The figure on the left shows a component of the proof for dimensions 3 and higher.\nReference: Itai Ashalgi, Mark Braverman, Avinatan Hassidim, Dov Monderer. Monotonicity and Implementability, Econometrica, 78(5), 2010.\nLinks: Econometrica 78(5), 2010; [pdf] [Supplementary material]\n"},{"id":11,"href":"/about/funding/","title":"Funding","parent":"About","content":"I gratefully acknowledge the following funding sources: Present:  NSF, through the Alan T. Waterman award CCF-1933331 The Simons Foundation, through the Simons Collaboration on Algorithms and Geometry The David and Lucile Packard Foundation, through a Packard Fellowship  Past:  NSF, through award CCF-1525342: \u0026ldquo;Noise across computational settings\u0026rdquo; Princeton University, through an Alfred Rheinstein Faculty Award NSF, through CAREER award CCF-1149888: \u0026ldquo;Coding and information theory for interactive computing\u0026rdquo; NSF, through award CCF-1215990: \u0026ldquo;Collaborative Research: Data-driven mechanisms in healthcare\u0026rdquo; (joint with Mohsen Bayati from Stanford) The John Templeton Foundation, through a Turing Fellowship, which was part of \u0026ldquo;The Turing Centenary Research Project - Mind, Mechanism and Mathematics\u0026rdquo; NSF, through the Center for Computational Intractability The Alfred P. Sloan Foundation, through a Sloan Research Fellowship NSERC, through a Discovery Grant \u0026ldquo;Complexity theory, algorithms and applications\u0026rdquo; (while at the University of Toronto).  "},{"id":12,"href":"/team/postdoc-opportunities/","title":"Postdoc opportunities","parent":"Group","content":"We have several exciting postdoctoral opportunities in Princeton and the area.\nPrinceton CS Theory group: Please see Princeton academic jobs portal here, and navigate to opportunities in Computer Science. The Theory Group typically advertises a postdoctoral research associate position and an associate research scholar position, which is slightly more senior. Both positions are reviewed in the late Fall, with the goal of making decisions by January/February.\nIAS Membership: More information here.\nDIMACS programs More information here\n"},{"id":13,"href":"/research/writings/","title":"Writings","parent":"Research","content":"An essay on the need for personal digital advocates (Last updated: December 2020)\nAbstract: There is a growing power imbalance between companies who have access to powerful algorithms and processing capacity and users who don’t. To restore the balance, we need to put equally powerful algorithms in the hands of individuals. There are two concrete steps we recommend:\n(1) there should be a new regulatory framework which creates an unbreakable commitment for an advocate (a digital service) to work exclusively in the interest of its client;\n(2) existing and new regulations around digital rights of individuals (such as GDPR) should make it a priority to make it easy for users to take advantage of these rights using software.\nFull essay: [pdf]\n"},{"id":14,"href":"/research/all-papers/","title":"All Papers","parent":"Research","content":"Selected recent publications   Network coding in undirected graphs is either very helpful or not helpful at all\nMark Braverman, Sumegha Garg, Ariel Schvartzman\n[arXiv]\n  Parallel algorithms for select and partition with noisy comparisons\nMark Braverman, Jieming Mao, Matt Weinberg\nSTOC'16; [arXiv]\n  Interpolating between truthful and non-truthful mechanisms for combinatorial auctions\nMark Braverman, Jieming Mao, Matt Weinberg\nSODA'16; [arXiv]\n  Constant-rate coding for multiparty interactive communication is impossible\nMark Braverman, Klim Efremenko, Ran Gelles, Bernhard Haeupler\nSTOC'16; [ECCC]\n  Data-driven incentive alignment in capitation schemes\nMark Braverman, Sylvain Chassang\nPreprint; [pdf]\n  Space-Bounded Church-Turing Thesis and computational tractability of closed systems\nMark Braverman, Cristobal Rojas, Jon Schneider\nPhys. Rev. Lett. 115, 098701; [link] [phys.org article]\n  Tight space-noise tradeoffs in computing the ergodic measure\nMark Braverman, Cristobal Rojas, Jon Schneider\n[arXiv]\n  Coding for interactive communication correcting insertions and deletions\nMark Braverman, Ran Gelles, Jieming Mao, Rafail Ostrovsky\nICALP'16; [arXiv]\n  Communication lower bounds for statistical estimation problems via a distributed data processing inequality\nMark Braverman, Ankit Garg, Tengyu Ma, Huy Nguyen, David Woodruff\nSTOC'16; [arXiv]\n  Near-optimal bounds on bounded-round quantum communication complexity of disjointness\nMark Braverman, Ankit Garg, Young Kun Ko, Jieming Mao, Dave Touchette\nFOCS'15; [ECCC]\n  ETH hardness for Densest-k-Subgraph with perfect completeness\nMark Braverman, Young Kun Ko, Aviad Rubinstein, Omri Weinstein\n[ECCC]\n  Information complexity is computable\nMark Braverman, Jon Schneider\nICALP'16; [ECCC]\nVideo of talk given at the Simons Institute [youtube]\n  Reliable communication over highly connected noisy networks\nNoga Alon, Mark Braverman, Klim Efremenko, Ran Gelles, Bernhard Haeupler\nPODC'16; [ECCC]\n  The communication complexity of Number-In-Hand Set Disjointness with no promise\nMark Braverman, Rotem Oshman\nPODC'15; [ECCC]\n  Simulating noisy channel interaction\nMark Braverman, Jieming Mao\nITCS'15; [ECCC]\n  Interactive information and coding theory\nMark Braverman\nA survey accompanying an invited lecture at ICM'14 in Seoul [pdf]\nVideo of the talk [youtube]\n  Small value parallel repetition for general games\nMark Braverman, Ankit Garg\nSTOC'15; [ECCC]\n  Approximating the best Nash equilibrium in -time breaks the Exponential Time Hypothesis\nMark Braverman, Young Kun Ko, Omri Weinstein\nSODA'15; [ECCC]\n  An interactive information odometer with applications\nMark Braverman, Omri Weinstein\nSTOC'15; [ECCC]\n  List and unique coding for interactive communication in the presence of adversarial noise\nMark Braverman, Klim Efremenko\nFOCS'14; [ECCC]\n  The computational hardness of pricing compound options\nMark Braverman, Kanika Pasricha\nworking paper; ITCS'14; [ECCC]\n  A hard-to-compress interactive task?\nMark Braverman\nAllerton'13; [pdf]\n  Public vs private coin in bounded-round information\nMark Braverman, Ankit Garg\nICALP'14; [ECCC]\n  Optimal Provision-After-Wait in healthcare\nMark Braverman, Jing Chen, Sampath Kannan\nworking paper; ITCS'14; [pdf]\n  Tight bounds for set disjointness in the message passing model\nMark Braverman, Faith Ellen, Rotem Oshman, Toniann Pitassi, Vinod Vaikuntanathan\nFOCS'13; [arXiv]\n  Direct products in communication complexity\nMark Braverman, Anup Rao, Omri Weinstein, Amir Yehudayoff\nFOCS'13; [ECCC]\n  From information to exact communication\nMark Braverman, Ankit Garg, Denis Pankratov, Omri Weinstein\nSTOC'13; [ECCC]\n  Direct product via round-preserving compression\nMark Braverman, Anup Rao, Omri Weinstein, Amir Yehudayoff\nICALP'13; [ECCC]\n  Information lower bounds via self-reducibility\nMark Braverman, Ankit Garg, Denis Pankratov, Omri Weinstein\nCSR'13; [ECCC]\nJournal of Computing Systems [Springer]\n  Search using queries on indistinguishable items\nMark Braverman, Gal Oshri\nSTACS'13; [arXiv]\n  On the convergence of the Hegselmann-Krause system\nArnab Bhattacharyya, Mark Braverman, Bernard Chazelle, Huy Nguyen\nITCS'13; [arXiv]\n  An information complexity approach to extended formulations\nMark Braverman, Ankur Moitra\nSTOC'13; [ECCC]\n  Coding for interactive computation: progress and challenges\nMark Braverman\nAllerton'12; [pdf]\n  Towards deterministic tree code constructions\nMark Braverman\nITCS'12; [ECCC]\n  Noise vs computational intractability in dynamics\nMark Braverman, Alexander Grigo, Cristobal Rojas\nITCS'12; [arXiv]\n  A discrepancy lower bound for information complexity\nMark Braverman, Omri Weinstein\nRANDOM'12; [arXiv]\n  Interactive information complexity\nMark Braverman\nSTOC'12; [ECCC]\n  The Grothendieck constant is strictly smaller than Krivine\u0026rsquo;s bound\nMark Braverman, Konstantin Makarychev, Yury Makarychev, Assaf Naor\nFOCS'11; [arXiv]\n  Towards coding for maximum errors in interactive communication\nMark Braverman, Anup Rao\nSTOC'11; [ECCC] [video by Anup]\n  Information equals amortized communication\nMark Braverman, Anup Rao\nFOCS'11; [ECCC]\n  Other publications   Leaky pseudo-entropy functions\nMark Braverman, Avinatan Hassidim, Yael Tauman Kalai\nIn ITCS'11; [pdf]\n  Finding endogenously formed communities\nMaria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, Shang-Hua Teng\nSODA'13; [arXiv]\n  Truthful mechanisms for competing submodular processes\nAllan Borodin, Mark Braverman, Brendan Lucier, Joel Oren\nWWW'13; [arXiv]\n  Inapproximability of NP-complete variants of Nash equilibrium\nPer Austrin, Mark Braverman, Eden Chlamtac\nin APPROX'11; [arXiv]\nTheory of Computing, 9(3), 2013.\n  Approximate Nash equilibria in perturbation resilient games\nNina Balcan, Mark Braverman\nWorking paper; [arXiv]\n  Pseudorandom Generators for Regular Branching Programs\nMark Braverman, Anup Rao, Ran Raz, Amir Yehudayoff\nFOCS'10; [ECCC] [video by Anup]\n  Computability of Brolin-Lyubich measure\nIlia Binder, Mark Braverman, Cristobal Rojas, Michael Yampolsky\nIn Comm. in Math. Phys.; [arXiv]\n  The rate of convergence of the Walk on Spheres Algorithm\nIlia Binder, Mark Braverman\nGeometric and Functional Analysis, to appear; [pdf]\n  Thurston equivalence to a rational map is decidable\nSylvain Bonnot, Mark Braverman, Michael Yampolsky\nMoscow Math. Journal, to appear; [arXiv]\n  How to compress interactive communication\nBoaz Barak, Mark Braverman, Xi Chen, Anup Rao\nSTOC'10 [pdf]\nPrevious version [ECCC]\n  Stability in large matching markets with complementarities\nItai Ashalgi, Mark Braverman, Avinatan Hassidim\nWorking paper; [pdf]\n  Phylogenetic Reconstruction with Insertions and Deletions\nAlex Andoni, Mark Braverman, Avinatan Hassidim\nWorking paper [pdf]\n  Monotonicity and Implementability\nItai Ashalgi, Mark Braverman, Avinatan Hassidim, Dov Monderer\nEconometrica 78(5), 2010; [pdf] [Supplementary material]\n  Sorting from Noisy Information\nMark Braverman, Elchanan Mossel\nSubmitted; [arXiv]\n  Ascending unit demand auctions with budget limits\nItai Ashalgi, Mark Braverman, Avinatan Hassidim\nWorking paper; [pdf]\n  Position Auctions with Budgets: Existence and Uniqueness\nItai Ashalgi, Mark Braverman, Avinatan Hassidim, Ron Lavi, Moshe Tennenholtz\nThe B.E. Journal of Theoretical Economics (Advances), 10(1), Article 20, 2010; [pdf].\n  Poly-logarithmic independence fools AC0 circuits\nMark Braverman\nComplexity'09; [ECCC]\nJournal of the ACM 57(5), 2010\nCommunications of the ACM, research highlight, 54(4), 2011\nLecture video from the 2011 METRIC workshop (audio quality not great): [video]\n  Finding low error clusterings\nNina Balcan, Mark Braverman\nCOLT'2009; [pdf]\n  The complexity of simulating Brownian Motion\nIlia Binder, Mark Braverman\nSODA'09; [pdf]\n  Constructing Locally Connected Non-Computable Julia Sets\nMark Braverman, Michael Yampolsky\nCommun. Math. Physics, 291(2), 2009; [pdf]\n  Space-Efficient Counting in Graphs on Surfaces\nMark Braverman, Raghav Kulkarni, Sambuddha Roy\nComputational Complexity 18(4), 2009; [pdf]\n  Pebbles and branching programs for tree evaluation\nStephen Cook, Pierre McKenzie, Dustin Wehr, Mark Braverman, Rahul Santhanam\nACM Transactions on Computation Theory (TOCT) 3(2), 2012; [arXiv]\nPreliminary version: MFCS'09 and FSTTCS'09\nSlides from Steve Cook\u0026rsquo;s talk with a \\$100 prize offer [pdf]\n  Book: Computability of Julia Sets\nMark Braverman, Michael Yampolsky\nSpringer, 2009.\n  Noisy Sorting Without Resampling\nMark Braverman, Elchanan Mossel\nSODA'08.\n  Mafia : a theoretical study of players and coalitions in a partial information environment\nMark Braverman, Omid Etesami, Elchanan Mossel\nAnnals of Appl. Prob. 18(3), 2008; [arXiv]\n  On ad hoc routing with guaranteed delivery\nMark Braverman\nBrief announcement, PODC'08; [arXiv]\n  The complexity of properly learning simple concept classes\nMisha Alekhnovich, Mark Braverman, Vitaly Feldman, Adam Klivans, Toniann Pitassi\nJournal of Computer and System Sciences, 74(1), 2008; [pdf]\n  Computability of Julia Sets\nMark Braverman, Michael Yampolsky\nMoscow Math. Journal 8(2), 2008; arXiv\n  Derandomizing Euclidean random walks\nIlia Binder, Mark Braverman\nRANDOM'07; [pdf]\n  Constructing Non-Computable Julia Sets\nMark Braverman, Michael Yampolsky\nSTOC'07; [pdf]\n  Parity Problems in Planar Graphs\nMark Braverman, Raghav Kulkarni, Sambuddha Roy\nComplexity'07; [pdf]\n  Filled Julia sets with empty interior are computable\nIlia Binder, Mark Braverman, Michael Yampolsky\nJournal of Found. of Comp. Math. 7(4), 2007; [arXiv]\n  On computational complexity of Riemann mapping\nIlia Binder, Mark Braverman, Michael Yampolsky\nArkiv for Matematik, 45(2), 2007; [arXiv]\n  Termination of Integer Linear Programs\nMark Braverman\nCAV'06 (Computer-Aided Verification); [pdf]\n  Non-Computable Julia Sets\nMark Braverman, Michael Yampolsky\nJourn. Amer. Math. Soc. 19(3), 2006; [arXiv]\n  Computing over the Reals: Foundations for Scientific Computing\nMark Braverman, Stephen Cook\nNotices of the AMS, 53(3), March 2006; [arXiv]\n  Parabolic Julia Sets are Polynomial Time Computable\nMark Braverman\nNonlinearity 19, 2006; [arXiv]\n  On computational complexity of Siegel Julia sets\nIlia Binder, Mark Braverman, Michael Yampolsky\nCommun. Math. Physics, 264(2), 2006; [arXiv]\n  On the Complexity of Real Functions\nMark Braverman\nFOCS'05 [pdf]\nFull version [arXiv]\n  Learnability and Automatizability\nMisha Alekhnovich, Mark Braverman, Vitaly Feldman, Adam Klivans, Toniann Pitassi\nFOCS'04; [pdf]\n  Hyperbolic Julia sets are poly-time computable\nMark Braverman\nCCA'04 (Computability and Complexity in Analysis), ENTCS 120; [pdf]\n  Other Manuscripts   Computational Complexity of Euclidean Sets: Hyperbolic Julia Sets are Poly-Time Computable\nMark Braverman\nMSc Thesis [pdf]\n  Computability and complexity of Julia sets\nMark Braverman\nPhD Thesis\n  Health care policy development and execution\nMohsen Bayati, Mark Braverman, Eric Horvitz, Michael Gillam\nUS patent application 20120004925; [USPTO]\n  Dispensing digital objects to an electronic wallet\nPhilipp Hertel, Alex Hertel, John Graham, Mark Braverman\nUS patent application 20110145049; [USPTO]\n  Secured electronic transaction system\nPhilipp Hertel, Alex Hertel, John Graham, Mark Braverman\nUS patent application 20090288012 [USPTO]\n  Predicting web advertisement click success by using head-to-head ratings\nMohsen Bayati, Mark Braverman, Satyen Kale, Yury Makarychev\nUS patent application 20100198685; [USPTO]\n  Method of registering and aligning multiple images\nVyacheslav Zavadsky, Jason Abt, Mark Braverman, Edward Keyes, Vladimir Martincevic\nSemiconductor Insights Inc.\nUS patent 7,693,348 [USPTO]\n  "},{"id":15,"href":"/research/","title":"Research","parent":"Mark Braverman","content":""},{"id":16,"href":"/team/","title":"Group","parent":"Mark Braverman","content":""},{"id":17,"href":"/team/undergraduate-projects/","title":"Undergraduate projects","parent":"Group","content":"2015-16:  Esther Rolf \u0026lsquo;16, Thesis, \u0026ldquo;Capturing multiparty communication: extending notions of information theory to topology-dependent multiparty problems\u0026rdquo; Michael Yitayew \u0026lsquo;16, Thesis, \u0026ldquo;Short-circuit error resilience in Boolean formulas\u0026rdquo; Zhongxia (Ricky) Zhao \u0026lsquo;16, Thesis, \u0026ldquo;Non-clearing and combinatorial Walrasian equilibria\u0026rdquo; Clement Lee \u0026lsquo;17, F15, \u0026ldquo;Fast polynomial factorization\u0026rdquo; Blair Wang \u0026lsquo;17, F15, \u0026ldquo;Discovery of spurious statistical relationships\u0026rdquo;  2014-15:  David Durst \u0026lsquo;15, Thesis, \u0026ldquo;Beyond3D: Visualizing high-dimensional data sets\u0026rdquo; James Pinkerton \u0026lsquo;15, Thesis, \u0026ldquo;Energy function arguments in finding Tower of Majority lower bounds\u0026rdquo;  2013-14:  Andra Constantinescu \u0026lsquo;14, Thesis, \u0026ldquo;Behavioral mechanism design - accounting for the mental cost of choosing\u0026rdquo; Mihai Roman \u0026lsquo;14, Thesis, \u0026ldquo;Computational complexity of pricing derivatives\u0026rdquo; Leonardo Stedile \u0026lsquo;14, Thesis, \u0026ldquo;Primal-dual based weights and other kidney exchange mechanisms\u0026rdquo; Valentina Barboy \u0026lsquo;15, F13, \u0026ldquo;An exploration of the accuracy and efficiency of peer grading\u0026rdquo; Stefani Karp \u0026lsquo;15, S14, \u0026ldquo;Information-theoretic approaches to the Unexpected Hanging Paradox\u0026rdquo; Christoph Schlom \u0026lsquo;15, S14, \u0026ldquo;Calibration testing\u0026rdquo;  2012-13:  Kevin Mantel \u0026lsquo;13, Thesis, \u0026ldquo;Election manipulation through misrepresentation of pre-election polls\u0026rdquo; Gal Oshri \u0026lsquo;13, Thesis, \u0026ldquo;Contracting experts with unknown cost structures\u0026rdquo; Kanika Pasricha, Thesis, \u0026ldquo;The computational hardness of pricing compound securities\u0026rdquo; Ashish Gupta \u0026lsquo;13, F12, \u0026ldquo;An investigation of the usefulness of data preprocessing techniques versus algorithmic sophistication on diabetes prediction\u0026rdquo; Leonardo Stedile \u0026lsquo;14, F12, \u0026ldquo;Analysis of risk in chess\u0026rdquo; Xiao Tian (Elaine) Liew \u0026lsquo;14, F12, \u0026ldquo;Effect of network structure on PageRank of spam\u0026rdquo; Xin Yang Yak \u0026lsquo;14, S13, \u0026ldquo;Do YouTube users upvote Better Comments? An analysis of the influence of spelling on comment ratings\u0026rdquo;  2011-12:  Michael Zhu \u0026lsquo;13, S12, \u0026ldquo;Finding overlapping communities in graphs\u0026rdquo; Gal Oshri \u0026lsquo;13, S12, \u0026ldquo;Search on multiple indistinguishable items\u0026rdquo; Rafael Grinberg \u0026lsquo;12, S12, \u0026ldquo;The effects of noise on cellular automata\u0026rdquo;  "},{"id":18,"href":"/","title":"Mark Braverman","parent":"","content":"\nI am a professor at the Department of Computer Science at Princeton University.\nMy office is 304 in CS Building (see campus map here).\nMy brief bio can be found here.\nI am interested in complexity theory, information theory, the theory of real computation, machine learning, algorithms, algorithmic mechanism design, and its applications.\nMy e-mail address: #######@cs.princeton.edu, replacing ####### with mbraverm.\n  Activities I co-organized a semester on the Nexus of Information and Computation Theories at the Institut Henri Poincaré in Paris, January–April 2016. For more information see here.\nI co-hosted a tutorial on information and communication complexity at ISIT'15 in Hong Kong.\nRecent and current program committees (since 2012): ITCS'17, FOCS'16, STOC'14, CCR'13, ITCS'13, RANDOM'12, FOCS'12.\nPapers All papers\nTeaching  Fall 2021: COS521 Advanced algorithm design  "},{"id":19,"href":"/posts/math-enabled/","title":"Math has been enabled","parent":"News","content":"Math is now enabled:\nInline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$; or\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n"},{"id":20,"href":"/posts/new-website/","title":"New Website","parent":"News","content":"I created my new website using Hugo and GeekDocs!\n"},{"id":21,"href":"/categories/","title":"Categories","parent":"Mark Braverman","content":""},{"id":22,"href":"/tags/","title":"Tags","parent":"Mark Braverman","content":""}]