[{"id":0,"href":"/posts/","title":"News","parent":"Mark Braverman","content":""},{"id":1,"href":"/about/","title":"About","parent":"Mark Braverman","content":""},{"id":2,"href":"/about/brief-bio/","title":"Brief Bio","parent":"About","content":"I received my PhD in 2008 from the Department of Computer Science at the University of Toronto under the supervision of Stephen Cook.\nIn 2007, together with Alex Hertel and Philipp Hertel, I co-founded Zetawire. It was acquired by Google from stealth mode in 2010, and formed the base for the Google Wallet product.\nBetween 2008 and 2010 I spent two years as a postdoctoral researcher at the Microsoft Research New England lab.\nIn 2010-11 I was an assistant professor jointly appointed at the Departments of Mathematics and Computer Science at the University of Toronto.\nI have been a professor of computer science at Princeton since 2015. I joined Princeton in 2011 as an assistant professor.\nWith my students and postdocs I work in complexity theory, the theory of real computation, machine learning, algorithms, game theory, and applications of computer science in healthcare and medicine. You can find my publications here. My research is supported by a number of awards, including a 2011 Sloan Fellowship and a 2013 Packard Fellowship.\n"},{"id":3,"href":"/team/members/","title":"Members","parent":"Group","content":"Current Group Members Graduate students:  Antonio Molina Lovett  Postdocs:  Vijay Bhattiprolu John Peebles Sahil Singla  Alumni: With first place of employment/study:\nGraduate students  Sumegha Garg PhD in Computer Science, Princeton, 2020; Rabin Postdoctoral Fellow at Harvard. Young Kun Ko PhD in Computer Science, Princeton, 2018; Assistant Professor / Faculty Fellow at NYU. Jieming Mao PhD in Computer Science, Princeton, 2018; Google Research New York. Jonathan Schneider, PhD in Computer Science, Princeton, 2018; Google Research New York. Ankit Garg, PhD in Computer Science, Princeton, 2016; Microsoft Research New England. Omri Weinstein, PhD in Computer Science, Princeton, 2015; NYU postdoc supported by the Simons Society of Fellows. Anastasios Zouzias, PhD in Computer Science, Toronto, 2013; IBM Research, Zurich. Wei Xi Fan, MSc in Mathematics, Toronto, 2011; Google Canada. Rinoc Johnson, MSc in Mathematics, Toronto, 2011; MapleSoft.  Postdocs I supervised or worked with closely  Shay Moran, 2017-2020, IAS, Princeton, and Google; Faculty at the Technion, Israel. Dor Mintzer, 2018-2020, IAS; Faculty at MIT. Sepehr Assadi, 2018-2019, Princeton; Faculty at Rutgers University. Gil Cohen, 2016-18, Princeton; Faculty at Tel-Aviv University, Israel. Ran Gelles, 2014-16, Princeton; Faculty at Bar-Ilan University, Israel. Matt Weinberg, 2014-16, Princeton; Faculty at Princeton University. Rotem Oshman, 2013-14, Princeton; Faculty at Tel-Aviv University, Israel. Klim Efremenko, 2012-13, IAS; Postdoc at the University of Chicago. Ankur Moitra, 2011-13, IAS; Faculty at MIT. Jing Chen, 2012-13, IAS; Faculty in Stony Brook University. Eden Chlamtac, 2010, Toronto; Faculty in Ben Gurion University, Be\u0026rsquo;er Sheva, Israel. Per Austrin, 2010-11, Toronto; Faculty in KTH, Stockholm, Sweden. Cristobal Rojas, 2010-11, Toronto; Faculty in Universidad Andres Bello, Santiago, Chile.  Undergraduate students See this list of past advised projects.\n"},{"id":4,"href":"/about/affiliations/","title":"Affiliations and service","parent":"About","content":" Princeton Computer Science Theory Group PACM (Program in Applied and Computational Mathematics) Simons Institute for the Theory of Computing Scientific Advisory Board Foundations of Computational Mathematics Board of Directors  "},{"id":5,"href":"/team/joining-the-group/","title":"Joining the group","parent":"Group","content":"Thank you for you interest in joining the group!\nUnfortunately, due to large volumes of email, I am not able to respond to individual inquiries about joining Princeton or our group. Some general details can be found below.\nHigh school and undergraduate students   If you are a Princeton undergrad, and are interested in the possibility of doing Junior/Senior independent work, please email me to make an appointment and discuss your interests. You can browse this page and look at the papers to get an idea of the topics we\u0026rsquo;re working on. Please also look at potential IW topics here.\n  I do not have internships for undergrads from outside Princeton. That said, truly exceptional undergrads (e.g. IMO/IOI medalists, top performance in Putnam, ACM programming world finalists, etc.) are welcome to contact me for potential opportunities.\n  Unfortunately, I do not have research opportunities for high school students.\n  If you have been admitted to the undergraduate program at Princeton, and are in the process of choosing between colleges and would like to meet, please send me an email. If you are considering or in the process of applying, I would prefer to wait until the admissions process runs its course before meeting. Note that I have no influence on the admissions decisions.\n  Prospective graduate students   We are recruiting outstanding graduate students!\n  If you are a current Princeton graduate student and would like to talk about the possibility of working with me, write me an email.\n  If you are not currently a graduate student at Princeton, you have to apply for admission first. More information can be found here. Please follow the procedure for applying. Please note that the selection of PhD students to be admitted is a competitive process based on the merits of individual applications.\n  You may also be interested in our funded Master\u0026rsquo;s program.\n  Emailing me does not improve your chances of being admitted. If anything, \u0026ldquo;mass\u0026rdquo; emails might slightly hurt your chances. If you are interested in working with me, please apply to the Theoretical Computer Science track and mention that you would like to work with me in your statement, and I will look at your file during the admissions process.\n  I cannot (nor am willing to try to) estimate your chances of being admitted to a graduate program in Princeton CS. The admissions process is complicated, and looks at a variety of factors in the application, including grades, personal statement, and recommendation letters.\n  If you feel that due to unusual circumstances your application is at a high risk for being overlooked, please have your academic adviser or senior program administrator send me a personal email.\n  Postdocs   We usually have a number of postdoc opportunities through the Theory Group, please see here.\n  If you feel that due to unusual circumstances your application is at a high risk for being overlooked, please have your PhD adviser send me a personal email.\n  "},{"id":6,"href":"/research/past-projects/","title":"Past projects","parent":"Research","content":"This page is under construction.\nTalks on complexity theory: A 15 minutes high level overview on theoretical computer science and information complexity: [pdf]\nNew bounds on the Grothendieck constant Let $A$ be an $n\\times n$ matrix. It gives rise to the quadratic form $$\\sum_{i,j} A_{ij} x_i y_j.$$ The Grothendieck constant $k$ bounds the ratio between the maximum of this form when $x_i,y_j$ are unit vectors in a Hilbert space and the maximum of this form when $x_i,y_j\\in {-1,1}$ are just real numbers. In other words, $k$ is the smallest number such that for all $A$,\n$$\\left|\\max_{||X_i||,||Y_j||\\leq 1} A_{ij} \\langle X_i, Y_j \\rangle\\right| \\leq k\\cdot \\left|\\max_{x_i,y_j\\in {-1,1}} A_{ij} x_i y_j\\right|.$$\nAlgorithmically, the Grothendieck constant can be viewed as the integrality gap between what can be attained by a general (vector) solution and the integral solution to the optimization problem of maximizing $\\sum_{i,j} A_{ij} x_i y_j$. A natural strategy for giving an upper bound on $k$ is to come up with a (possibly randomized) _rounding scheme_ that converts vectors $X_i, Y_j$ into numbers $x_i,y_j$ while losing a factor of at most $k$ in the objective function.\nRounding vectors to numbers has had many important applications in algorithms. For example, the Goemans-Williamson MAX CUT approximation algorithm represents the MAXCUT instance as a (tractable) optimization problem over vectors, and then uses a random projection onto a line to obtain an approximate integral solution. The approximation ratio is exactly the worst-case ratio between the value attained by the vector solution versus the integral solution.\nIn 1977, Krivine proposed a natural rounding scheme for the Grothendieck inequality, proving an upper bound $k\\le c_k= \\frac{\\pi}{2\\ln (1+\\sqrt{2})}\\approx 1.7822$, which he conjectured to be tight. The scheme (similarly to the rounding schemes in the algorithms from decades to follow), involved transforming the vectors and then applying a random projection of the vectors to a line (with vectors projected to the positive side assigned a $+1$, and those to the negative side a $-1$). In 2011 we disproved Krivine\u0026rsquo;s conjecture that $k=c_k$ by showing that $k\u0026lt;c_k$.\nThis is done by devising a better rounding scheme (or, rather showing that one exists via a delicate perturbation argument). In the process, we show that, most likely, a two-dimensional rounding strategy can beat the one dimensional strategy of projecting vectors onto a line. The two-dimensional strategy would partition the plane into two regions $R_{+1}$ and $R_{-1}$, and map vectors to numbers by projecting them to the plane and then assigning them to $\\pm 1$ based on the region into which they fall. The one-dimensional strategy corresponds to $R_{+1}$ and $R_{-1}$ each being a half-plane.\nThe diagram on the right gives an educated guess on what the partition of the plane $\\mathbb R^2$ might look like for the optimal two-dimensional Krivine scheme. If Krivine\u0026rsquo;s conjecture were true, the optimal partition would have been just a half-plane.\nReference: Mark Braverman, Konstantin Makarychev, Yury Makarychev, Assaf Naor. The Grothendieck constant is strictly smaller than Krivine\u0026rsquo;s bound, 2011.\nLinks: paper on arXiv; FOCS'11 version from IEEE; open access version from Forum of Mathematics, Pi.\nComputability and complexity of Julia sets Monotonicity and implementability Consider an environment with a finite set of alternatives $A_1,\\ldots,A_k$ and agents who have quasi-linear preferences. That is, each agent $i$ has utility $u_{ij}$ for alternative $A_j$, and if she has to pay an amount $p_i$ her utility becomes $u_{ij}-p_i$. A direct mechanism consists of an allocation rule $f$ and payment rule $p$. The allocation rule maps each profile of valuations to a probability vector over the set of possible alternatives. For example, the second price auction on one item amounts to the allocation rule being “the highest bidder gets the item”, and the payment collected from the winner equals the second-highest bid (and no payment collected from other players).\nAn allocation rule $f$ is said to be implementable if there exists a payment rule $p$ so it is safe for each agent to report his true valuation to the mechanism $(f,p)$\u0026quot;) regardless of the reports of all other agents. In other words, the payment rule $p$ makes truthful reporting a dominant strategy for all players. Understanding which allocation rules are implementable is a fundamental concern in mechanism design. Myerson has shown in his seminal 1981 paper, that when the set of alternatives is single dimensional (for example when a single item is auctioned for sale), an allocation rule is implementable if an only if it is monotone in the valuation of a player. Informally, monotonicity means that higher demand for an outcome by a player will result in a greater-or-equal allocation of that outcome.\nFor multidimensional settings, such as selling multiple goods, Rochet has shown in 1987 that a stronger condition, called cycle-monotonicity, of an allocation rule is a necessary and sufficient condition for implementing it. Cyclic monotonicity however is a considerably more difficult condition to work with than monotonicity. Monotonicity is a condition on every pair of values, whereas cyclic monotonicity is a condition on every finite sequence of values. This gives rise to the natural question of when does monotonicity imply cycle-monotonicity (and thus implementability).\nSaks and Yu (2005) have shown that when the domain of (multidimensional) valuations is convex, monotonicity is necessary and sufficient for implementing an allocation rule. We show that convexity is also necessary in the following sense: if the domain of valuation is not convex, there always exists an allocation rule that is monotone yet not implementable. In other words, when the domain of possible valuations is not convex, local constraints alone are not sufficient to guarantee implementability.\nFor each non-convex domain, the proof involves the construction of an allocation rule that is not implementable. The figure on the left shows a component of the proof for dimensions 3 and higher.\nReference: Itai Ashalgi, Mark Braverman, Avinatan Hassidim, Dov Monderer. Monotonicity and Implementability, Econometrica, 78(5), 2010.\nLinks: Econometrica 78(5), 2010; [pdf] [Supplementary material]\n"},{"id":7,"href":"/research/recent-projects/","title":"Recent projects","parent":"Research","content":"Information complexity Information theory is a vastly successful theory underpinning much of our communication technology. In many important communication scenarios it allows one to calculate the precise amount of resources needed to perform a cetrain task. For example, the number of bits Alice needs to communicate to send a string $x_1 x_2 \\ldots x_n$ of $n$ random trits (elements from $\\lbrace 1,2,3\\rbrace$) to Bob is given by $(\\log_2 3) \\cdot n \\pm o(n)$. The number of bits Alice needs to communicate to send $n$ random trits to Bob if Bob already knows half of the $x$\u0026rsquo;s (regarless of whether Alice knows which locations are known to Bob) is $\\frac{\\log_2 3}{2}\\cdot n \\pm o(n)$ etc. These quantities can be written as expressions in terms of things like Shannon\u0026rsquo;s entorpy, mutual information, and conditional mutual information.\nNotions of entropy and mutual information allow to precisely (and losslessly!) describe the flow of information in a variety of settings. They also allow to one to write \u0026ldquo;common sense\u0026rdquo; statements about information in precise mathematical terms which can then be manipulated. This turns out to be extremely useful in reasoning about communication. For example entropy $H(M)$ represents the amount of uncertainty in a message $M$. Mutual information $I(M;X)$ represents the amount of information the message $M$ reveals about a piece of data $X$. Conditional mutual information $I(M; X|Y)$ represents the amount of information a message $M$ reveals about $X$ to someone who already knows a piece of data $Y$.\nThe chain rule $$ I(M_1 M_2; X) = I(M_1; X) + I(M_2; X|M_1) $$ is just a precise way to formalize the intuitive fact that what one learned from two messages $M_1 M_2$ about $X$ can be decomposed into what one learned from the first message plus what one learned from the second message when we already knew the first one. The data processing inequality $$ I(X;F(Y)) \\le I(X;Y) $$ formalizes the intuition that a function computed on $Y$ cannot reveal more about $X$ than $Y$ itself.\nThe goal of information complexity is to learn to apply information-theoretic formalism to computational settings. As of now (2021), these tend to work beautifully in models of computation that are simple enough to not support information-theoretically secure computation. Understanding why exactly this happens is an interesting direction for future work. Below we give some examples of results based on information complexity from the 2010-2020 period. In some cases the results are new, while in others they give conceptually simpler proofs or more general statements of existing theorems.\nTwo party communication In the two-party communication complexity setting, two players (Alice and Bob) are given inputs $X$ and $Y$. They are also allowed to use a randomness source $R$. A protocol $\\pi$ is just a formalization of a conversation: each message in $\\pi$ is allowed to depend on the speaker\u0026rsquo;s input, on the conversation so far, and on the public randomness. The communication cost of a protocol is the number of bits communicated during its execution. The communication complexity $CC(T)$ of a task $T$ is the smallest communication cost of a protocol $\\pi$ solving $T$. In the context of communication complexity, $T$ is typically the task of \u0026ldquo;computing a given function $F(X,Y)$ with error probability $\u0026lt;\\varepsilon$\u0026rdquo;.\nAn instructive example is the Equality problem. Alice is given an $n$-bit string $X$, Bob is given an $n$-bit string $Y$, and they would like to determine whether $X=Y$. It can be shown that this can be accomplished with error $\u0026lt;\\varepsilon$ using $k \\sim \\log (1/\\varepsilon)$ bits of communication. Alice will compute a random hash $h(X)$ of lenght $k$ on her input $X$ and send the value to Bob. Bob will compare $h(X)$ to $h(Y)$, and will return \u0026lsquo;equal\u0026rsquo; if they match. There is a $2^{-k}$ probability of hash collision, which means that \u0026lsquo;equal\u0026rsquo; is returned with probability $\\approx 2^{-k}$ even when $X\\neq Y$. Interestingly, a zero-error protocol for equality requires $n+1$ bits of communication.\nAnother function with many applications in lower bounds is Disjointness. Alice and Bob are given subsets $X,Y\\subset \\lbrace 1,\\ldots,n\\rbrace$ (wich can be represented as length-$n$ bit-strings), and need to determine whether they have an element in common. $Disj_n(X,Y)=1$ if the sets are disjoint, and $0$ if they intersect.\nDirect sum and information complexity The direct sum problem (in any model of computation) asks whether it costs $k$ times as much to perform $k$ independent copies of a task $T$ as it costs to perform one copy, or whether one gets a \u0026ldquo;volume discount\u0026rdquo;. When direct sum holds, one gets a powerful lower bounds tool \u0026mdash; a lower bound $L$ on $T$ gets amplified into a lower bound of $k\\cdot L$ on the task $T^k$ of performing $k$ copies of $T$. When direct sum fails, new interesting algorithms follow. A famous example of such a failure is matrix-vector multiplication. A counting argument shows that one needs $\u0026gt;n^2$ operations to mutiply an $n\\times n$ matrix $A$ by a vector $v$. At the same time, multiplying $A$ by $n$ different vectors $v_1,\\ldots,v_n$ is just the problem of multiplying two $n\\times n$ matrices, which can be done faster than $n\\cdot n^2=n^3$ via fast matrix multiplication.\nThe Direct Sum Problem for randomized communication complexity asked whether the direct sum property holds for randomized communication complexity. That is, whether $CC(T^k)\\gtrsim k\\cdot CC(T)$? It turns out that trying to answer this question is closely related to understanding the information complexity of $F$.\nInformation complexity is defined similarly to communication complexity, with information cost replacing communication cost. The (two-party) information cost of a protocol $\\Pi$ on inputs $(X,Y)$ is defined to be the amount of information the participants learn about each other\u0026rsquo;s inputs during the execution of the protocol. Fortunately, the information-theoretic notation makes this quantity easy to formalize: $$ IC(\\Pi):= I(X;\\Pi|Y) + I(Y;\\Pi|X). $$ The first term corresponds to what Bob (who knows $Y$) learns about Alice\u0026rsquo;s input ($X$) from the protocol $\\Pi$. The second term corresponds to what Alice learns. The information complexity of a task $T$ over inputs $(X,Y)$ is defined as the smallest possible information cost of a protocol solving $T$. $$ IC(T):= \\inf_{\\text{$\\Pi$ solves $T$}} IC(\\Pi). $$ The infimum here is necessary, since it is possible that there is a sequence of successful protocols $\\Pi_1,\\Pi_2,\\ldots$ that get ever longer while revealing an ever smaller amount of information (in fact, this happens for the simple task of computing the two-bit AND function).\nIt turns out that the amortized (per-copy) communication complexity of a task $T$ is equal to its information complexity, at least when a vanishing amount of error is allowed. Let $T(X,Y)$ be a task that allows for a small amount of error $\\ve=o(1)$. Let $CC(T^k)$ be the communication complexity of $k$ copies of $T$, and let $IC(T)$ be its information complexity. Then information is equal to amortized communication:\nTheorem [BR'11]: $\\displaystyle{\\lim_{k\\rightarrow\\infty} \\frac{CC(T^k)}{k} = IC(T)}$.\nInteractive compression This theorem gives an immediate blueprint for proving direct sum theorems for (randomized) communication complexity. To show (for example) that $CC(T^k) \\gtrsim k\\cdot CC(T^k)$, one can show that $IC(T)\\gtrsim CC(T)$. This inequality, more commonly written as $CC(T)\\lesssim IC(T)$ is known as the interactive compression question. It asks whether a protocol $\\pi$ that solves $T$ while revealing little information can be \u0026ldquo;compressed\u0026rdquo; into a protocol $\\pi'$ that uses little communication.\nIn the context of one-way communication near-perfect compression is generally possibly. For example Huffman coding allows one to encode a message with entropy $H(M)$ using at most $H(M)+1$ bits in expectation. It is generally the case that a protocol with $r$ rounds of communication can be compressed into $O(IC(\\Pi)+r)$ bits of communication. Unfortunately, the protocol achieving $IC(T)$ may have an unbounded number of rounds, making such a compression useless in the general interactive setting.\nIt turns out that it is possible to compress a general protocol $\\pi$ whose information cost is $I$ and whose communication cost is $C$ into a protocol $\\pi'$ whose communication cost is $\\tilde{O}(\\sqrt{I\\cdot C})$. This leads to a partial direct sum theorem for randomized communication:\nTheorem [BBCR'10]: $\\displaystyle{CC(T^k)}=\\tilde{\\Omega}(\\sqrt{k}\\cdot CC(T))$.\nAt the same time, Ganor, Kol, and Raz showed a separation between information and communication complexity:\nTheorem [GKR'15]: There is a family of functions whose information complexity is $n$ and whose communication complexity is $2^{\\Omega(n)}$.\nMoreover, it can be shown that this separation is tight - communication complexity is always at most exponential in information complexity. The GKR'15 theorem rules out a tight direct sum theorem for communication complexity. It still remains open whether the $\\sqrt{k}$ in BBCR'10 can be improved, e.g. to $k^{1-\\varepsilon}$.\nDirect product and parallel repetition: using information formalism In light of the BR'11 and GKR'15 results, information complexity becomes the \u0026ldquo;correct\u0026rdquo; measure for studying amortized cost of two-player randomized communication complexity. The connection from BR'11 between information and amortized communication can be further deepened by a direct product theorem. A direct sum theorem gives a lower bound on the amount of resources required to accomplish $k$ copies of a task. A direct product theorem further says that trying to solve $k$ copies of the task with fewere resources will fail except with an exponentially small probability.\nTheorem [BRWY'13,BW'14]: If $I$ bits of information are required to compute a single copy of a function $f(x,y)$ under an input distribution $\\mu$, then any communication protocol that attempts to solve $k$ copies of $f$ with input distributed according to $\\mu^k$ using $o(k\\cdot I)$ bits of communication will fail except with an exponentially small probabiliy $2^{-\\Omega(k)}$.\nAt the heart of the proof of the direct product theorem is applying information-theoretic formalism which treats the \u0026ldquo;success event\u0026rdquo; as just another piece of information. Formalism such as the chain rule allows to make statements of the form \u0026ldquo;succeeding on the first copy does not reveal too much about inputs to the second copy\u0026rdquo; formal.\nPreviously, such reasoning has been used successfully to prove the parallel repetition theorem for two-prover games. Two-prover games warrant a separate discussion, but it should be mentioned that formalism from the direct product theorem from communication complexity can be lifted to re-prove the parallel repetition theorem, and give the only known proof of the parallel repetition theorem in the low-success-probability regime [BG'15].\nThe two-bit AND function and Disjointness Turning from abstract complexity-theoretic results to concrete ones, let us consider the information complexity of specific functions. The simplest functions are ones are of the form $f:\\lbrace 0,1\\rbrace\\times\\lbrace 0,1\\rbrace \\rightarrow \\lbrace 0,1\\rbrace$, where Alice and Bob are each given a single bit of input. Of those, only the AND function (and equivalent transformations) is interesting from the information complexity perspective. Other functions are either constant, amount to one-way data transmission (projection functions), or to a two-way data-exchange (the XOR function).\nGenerally speaking, we do not know an efficient procedure for computing the information complexity of a function from its truth table. In fact, even proving that information complexity is computable appears to be non-trivial [BS'16]. Fortunately, in the case of the two-bit AND, it is possible to describe the information-theoretically optimal protocol.\nGenerally speaking, the optimal protocol depends not only on the function $f$ being computed but also on the prior distribution $\\mu$ of inputs - this is because the amount of information revealed by a protocol $\\pi$ depends on the prior distribution $\\mu$.\nDoes one need memory to approximately count? Beyond two party communication? Further reading: a survey can be found here. Quantum information complexity. Parallel repetition.\n"},{"id":8,"href":"/about/funding/","title":"Funding","parent":"About","content":"I gratefully acknowledge the following funding sources: Present:  NSF, through the Alan T. Waterman award CCF-1933331 The Simons Foundation, through the Simons Collaboration on Algorithms and Geometry The David and Lucile Packard Foundation, through a Packard Fellowship  Past:  NSF, through award CCF-1525342: \u0026ldquo;Noise across computational settings\u0026rdquo; Princeton University, through an Alfred Rheinstein Faculty Award NSF, through CAREER award CCF-1149888: \u0026ldquo;Coding and information theory for interactive computing\u0026rdquo; NSF, through award CCF-1215990: \u0026ldquo;Collaborative Research: Data-driven mechanisms in healthcare\u0026rdquo; (joint with Mohsen Bayati from Stanford) The John Templeton Foundation, through a Turing Fellowship, which was part of \u0026ldquo;The Turing Centenary Research Project - Mind, Mechanism and Mathematics\u0026rdquo; NSF, through the Center for Computational Intractability The Alfred P. Sloan Foundation, through a Sloan Research Fellowship NSERC, through a Discovery Grant \u0026ldquo;Complexity theory, algorithms and applications\u0026rdquo; (while at the University of Toronto).  "},{"id":9,"href":"/team/postdoc-opportunities/","title":"Postdoc opportunities","parent":"Group","content":"We have several exciting postdoctoral opportunities in Princeton and the area.\nPrinceton CS Theory group: Please see Princeton academic jobs portal here, and navigate to opportunities in Computer Science. The Theory Group typically advertises a postdoctoral research associate position and an associate research scholar position, which is slightly more senior. Both positions are reviewed in the late Fall, with the goal of making decisions by January/February.\nIAS Membership: More information here.\nDIMACS programs More information here\n"},{"id":10,"href":"/research/writings/","title":"Writings","parent":"Research","content":"An essay on the need for personal digital advocates (Last updated: December 2020)\nAbstract: There is a growing power imbalance between companies who have access to powerful algorithms and processing capacity and users who don’t. To restore the balance, we need to put equally powerful algorithms in the hands of individuals. There are two concrete steps we recommend:\n(1) there should be a new regulatory framework which creates an unbreakable commitment for an advocate (a digital service) to work exclusively in the interest of its client;\n(2) existing and new regulations around digital rights of individuals (such as GDPR) should make it a priority to make it easy for users to take advantage of these rights using software.\nFull essay: [pdf]\n"},{"id":11,"href":"/research/all-papers/","title":"All Papers","parent":"Research","content":"Selected recent publications   Network coding in undirected graphs is either very helpful or not helpful at all\nMark Braverman, Sumegha Garg, Ariel Schvartzman\n[arXiv]\n  Parallel algorithms for select and partition with noisy comparisons\nMark Braverman, Jieming Mao, Matt Weinberg\nSTOC'16; [arXiv]\n  Interpolating between truthful and non-truthful mechanisms for combinatorial auctions\nMark Braverman, Jieming Mao, Matt Weinberg\nSODA'16; [arXiv]\n  Constant-rate coding for multiparty interactive communication is impossible\nMark Braverman, Klim Efremenko, Ran Gelles, Bernhard Haeupler\nSTOC'16; [ECCC]\n  Data-driven incentive alignment in capitation schemes\nMark Braverman, Sylvain Chassang\nPreprint; [pdf]\n  Space-Bounded Church-Turing Thesis and computational tractability of closed systems\nMark Braverman, Cristobal Rojas, Jon Schneider\nPhys. Rev. Lett. 115, 098701; [link] [phys.org article]\n  Tight space-noise tradeoffs in computing the ergodic measure\nMark Braverman, Cristobal Rojas, Jon Schneider\n[arXiv]\n  Coding for interactive communication correcting insertions and deletions\nMark Braverman, Ran Gelles, Jieming Mao, Rafail Ostrovsky\nICALP'16; [arXiv]\n  Communication lower bounds for statistical estimation problems via a distributed data processing inequality\nMark Braverman, Ankit Garg, Tengyu Ma, Huy Nguyen, David Woodruff\nSTOC'16; [arXiv]\n  Near-optimal bounds on bounded-round quantum communication complexity of disjointness\nMark Braverman, Ankit Garg, Young Kun Ko, Jieming Mao, Dave Touchette\nFOCS'15; [ECCC]\n  ETH hardness for Densest-k-Subgraph with perfect completeness\nMark Braverman, Young Kun Ko, Aviad Rubinstein, Omri Weinstein\n[ECCC]\n  Information complexity is computable\nMark Braverman, Jon Schneider\nICALP'16; [ECCC]\nVideo of talk given at the Simons Institute [youtube]\n  Reliable communication over highly connected noisy networks\nNoga Alon, Mark Braverman, Klim Efremenko, Ran Gelles, Bernhard Haeupler\nPODC'16; [ECCC]\n  The communication complexity of Number-In-Hand Set Disjointness with no promise\nMark Braverman, Rotem Oshman\nPODC'15; [ECCC]\n  Simulating noisy channel interaction\nMark Braverman, Jieming Mao\nITCS'15; [ECCC]\n  Interactive information and coding theory\nMark Braverman\nA survey accompanying an invited lecture at ICM'14 in Seoul [pdf]\nVideo of the talk [youtube]\n  Small value parallel repetition for general games\nMark Braverman, Ankit Garg\nSTOC'15; [ECCC]\n  Approximating the best Nash equilibrium in -time breaks the Exponential Time Hypothesis\nMark Braverman, Young Kun Ko, Omri Weinstein\nSODA'15; [ECCC]\n  An interactive information odometer with applications\nMark Braverman, Omri Weinstein\nSTOC'15; [ECCC]\n  List and unique coding for interactive communication in the presence of adversarial noise\nMark Braverman, Klim Efremenko\nFOCS'14; [ECCC]\n  The computational hardness of pricing compound options\nMark Braverman, Kanika Pasricha\nworking paper; ITCS'14; [ECCC]\n  A hard-to-compress interactive task?\nMark Braverman\nAllerton'13; [pdf]\n  Public vs private coin in bounded-round information\nMark Braverman, Ankit Garg\nICALP'14; [ECCC]\n  Optimal Provision-After-Wait in healthcare\nMark Braverman, Jing Chen, Sampath Kannan\nworking paper; ITCS'14; [pdf]\n  Tight bounds for set disjointness in the message passing model\nMark Braverman, Faith Ellen, Rotem Oshman, Toniann Pitassi, Vinod Vaikuntanathan\nFOCS'13; [arXiv]\n  Direct products in communication complexity\nMark Braverman, Anup Rao, Omri Weinstein, Amir Yehudayoff\nFOCS'13; [ECCC]\n  From information to exact communication\nMark Braverman, Ankit Garg, Denis Pankratov, Omri Weinstein\nSTOC'13; [ECCC]\n  Direct product via round-preserving compression\nMark Braverman, Anup Rao, Omri Weinstein, Amir Yehudayoff\nICALP'13; [ECCC]\n  Information lower bounds via self-reducibility\nMark Braverman, Ankit Garg, Denis Pankratov, Omri Weinstein\nCSR'13; [ECCC]\nJournal of Computing Systems [Springer]\n  Search using queries on indistinguishable items\nMark Braverman, Gal Oshri\nSTACS'13; [arXiv]\n  On the convergence of the Hegselmann-Krause system\nArnab Bhattacharyya, Mark Braverman, Bernard Chazelle, Huy Nguyen\nITCS'13; [arXiv]\n  An information complexity approach to extended formulations\nMark Braverman, Ankur Moitra\nSTOC'13; [ECCC]\n  Coding for interactive computation: progress and challenges\nMark Braverman\nAllerton'12; [pdf]\n  Towards deterministic tree code constructions\nMark Braverman\nITCS'12; [ECCC]\n  Noise vs computational intractability in dynamics\nMark Braverman, Alexander Grigo, Cristobal Rojas\nITCS'12; [arXiv]\n  A discrepancy lower bound for information complexity\nMark Braverman, Omri Weinstein\nRANDOM'12; [arXiv]\n  Interactive information complexity\nMark Braverman\nSTOC'12; [ECCC]\n  The Grothendieck constant is strictly smaller than Krivine\u0026rsquo;s bound\nMark Braverman, Konstantin Makarychev, Yury Makarychev, Assaf Naor\nFOCS'11; [arXiv]\n  Towards coding for maximum errors in interactive communication\nMark Braverman, Anup Rao\nSTOC'11; [ECCC] [video by Anup]\n  Information equals amortized communication\nMark Braverman, Anup Rao\nFOCS'11; [ECCC]\n  Other publications   Leaky pseudo-entropy functions\nMark Braverman, Avinatan Hassidim, Yael Tauman Kalai\nIn ITCS'11; [pdf]\n  Finding endogenously formed communities\nMaria-Florina Balcan, Christian Borgs, Mark Braverman, Jennifer Chayes, Shang-Hua Teng\nSODA'13; [arXiv]\n  Truthful mechanisms for competing submodular processes\nAllan Borodin, Mark Braverman, Brendan Lucier, Joel Oren\nWWW'13; [arXiv]\n  Inapproximability of NP-complete variants of Nash equilibrium\nPer Austrin, Mark Braverman, Eden Chlamtac\nin APPROX'11; [arXiv]\nTheory of Computing, 9(3), 2013.\n  Approximate Nash equilibria in perturbation resilient games\nNina Balcan, Mark Braverman\nWorking paper; [arXiv]\n  Pseudorandom Generators for Regular Branching Programs\nMark Braverman, Anup Rao, Ran Raz, Amir Yehudayoff\nFOCS'10; [ECCC] [video by Anup]\n  Computability of Brolin-Lyubich measure\nIlia Binder, Mark Braverman, Cristobal Rojas, Michael Yampolsky\nIn Comm. in Math. Phys.; [arXiv]\n  The rate of convergence of the Walk on Spheres Algorithm\nIlia Binder, Mark Braverman\nGeometric and Functional Analysis, to appear; [pdf]\n  Thurston equivalence to a rational map is decidable\nSylvain Bonnot, Mark Braverman, Michael Yampolsky\nMoscow Math. Journal, to appear; [arXiv]\n  How to compress interactive communication\nBoaz Barak, Mark Braverman, Xi Chen, Anup Rao\nSTOC'10 [pdf]\nPrevious version [ECCC]\n  Stability in large matching markets with complementarities\nItai Ashalgi, Mark Braverman, Avinatan Hassidim\nWorking paper; [pdf]\n  Phylogenetic Reconstruction with Insertions and Deletions\nAlex Andoni, Mark Braverman, Avinatan Hassidim\nWorking paper [pdf]\n  Monotonicity and Implementability\nItai Ashalgi, Mark Braverman, Avinatan Hassidim, Dov Monderer\nEconometrica 78(5), 2010; [pdf] [Supplementary material]\n  Sorting from Noisy Information\nMark Braverman, Elchanan Mossel\nSubmitted; [arXiv]\n  Ascending unit demand auctions with budget limits\nItai Ashalgi, Mark Braverman, Avinatan Hassidim\nWorking paper; [pdf]\n  Position Auctions with Budgets: Existence and Uniqueness\nItai Ashalgi, Mark Braverman, Avinatan Hassidim, Ron Lavi, Moshe Tennenholtz\nThe B.E. Journal of Theoretical Economics (Advances), 10(1), Article 20, 2010; [pdf].\n  Poly-logarithmic independence fools AC0 circuits\nMark Braverman\nComplexity'09; [ECCC]\nJournal of the ACM 57(5), 2010\nCommunications of the ACM, research highlight, 54(4), 2011\nLecture video from the 2011 METRIC workshop (audio quality not great): [video]\n  Finding low error clusterings\nNina Balcan, Mark Braverman\nCOLT'2009; [pdf]\n  The complexity of simulating Brownian Motion\nIlia Binder, Mark Braverman\nSODA'09; [pdf]\n  Constructing Locally Connected Non-Computable Julia Sets\nMark Braverman, Michael Yampolsky\nCommun. Math. Physics, 291(2), 2009; [pdf]\n  Space-Efficient Counting in Graphs on Surfaces\nMark Braverman, Raghav Kulkarni, Sambuddha Roy\nComputational Complexity 18(4), 2009; [pdf]\n  Pebbles and branching programs for tree evaluation\nStephen Cook, Pierre McKenzie, Dustin Wehr, Mark Braverman, Rahul Santhanam\nACM Transactions on Computation Theory (TOCT) 3(2), 2012; [arXiv]\nPreliminary version: MFCS'09 and FSTTCS'09\nSlides from Steve Cook\u0026rsquo;s talk with a \\$100 prize offer [pdf]\n  Book: Computability of Julia Sets\nMark Braverman, Michael Yampolsky\nSpringer, 2009.\n  Noisy Sorting Without Resampling\nMark Braverman, Elchanan Mossel\nSODA'08.\n  Mafia : a theoretical study of players and coalitions in a partial information environment\nMark Braverman, Omid Etesami, Elchanan Mossel\nAnnals of Appl. Prob. 18(3), 2008; [arXiv]\n  On ad hoc routing with guaranteed delivery\nMark Braverman\nBrief announcement, PODC'08; [arXiv]\n  The complexity of properly learning simple concept classes\nMisha Alekhnovich, Mark Braverman, Vitaly Feldman, Adam Klivans, Toniann Pitassi\nJournal of Computer and System Sciences, 74(1), 2008; [pdf]\n  Computability of Julia Sets\nMark Braverman, Michael Yampolsky\nMoscow Math. Journal 8(2), 2008; arXiv\n  Derandomizing Euclidean random walks\nIlia Binder, Mark Braverman\nRANDOM'07; [pdf]\n  Constructing Non-Computable Julia Sets\nMark Braverman, Michael Yampolsky\nSTOC'07; [pdf]\n  Parity Problems in Planar Graphs\nMark Braverman, Raghav Kulkarni, Sambuddha Roy\nComplexity'07; [pdf]\n  Filled Julia sets with empty interior are computable\nIlia Binder, Mark Braverman, Michael Yampolsky\nJournal of Found. of Comp. Math. 7(4), 2007; [arXiv]\n  On computational complexity of Riemann mapping\nIlia Binder, Mark Braverman, Michael Yampolsky\nArkiv for Matematik, 45(2), 2007; [arXiv]\n  Termination of Integer Linear Programs\nMark Braverman\nCAV'06 (Computer-Aided Verification); [pdf]\n  Non-Computable Julia Sets\nMark Braverman, Michael Yampolsky\nJourn. Amer. Math. Soc. 19(3), 2006; [arXiv]\n  Computing over the Reals: Foundations for Scientific Computing\nMark Braverman, Stephen Cook\nNotices of the AMS, 53(3), March 2006; [arXiv]\n  Parabolic Julia Sets are Polynomial Time Computable\nMark Braverman\nNonlinearity 19, 2006; [arXiv]\n  On computational complexity of Siegel Julia sets\nIlia Binder, Mark Braverman, Michael Yampolsky\nCommun. Math. Physics, 264(2), 2006; [arXiv]\n  On the Complexity of Real Functions\nMark Braverman\nFOCS'05 [pdf]\nFull version [arXiv]\n  Learnability and Automatizability\nMisha Alekhnovich, Mark Braverman, Vitaly Feldman, Adam Klivans, Toniann Pitassi\nFOCS'04; [pdf]\n  Hyperbolic Julia sets are poly-time computable\nMark Braverman\nCCA'04 (Computability and Complexity in Analysis), ENTCS 120; [pdf]\n  Other Manuscripts   Computational Complexity of Euclidean Sets: Hyperbolic Julia Sets are Poly-Time Computable\nMark Braverman\nMSc Thesis [pdf]\n  Computability and complexity of Julia sets\nMark Braverman\nPhD Thesis\n  Health care policy development and execution\nMohsen Bayati, Mark Braverman, Eric Horvitz, Michael Gillam\nUS patent application 20120004925; [USPTO]\n  Dispensing digital objects to an electronic wallet\nPhilipp Hertel, Alex Hertel, John Graham, Mark Braverman\nUS patent application 20110145049; [USPTO]\n  Secured electronic transaction system\nPhilipp Hertel, Alex Hertel, John Graham, Mark Braverman\nUS patent application 20090288012 [USPTO]\n  Predicting web advertisement click success by using head-to-head ratings\nMohsen Bayati, Mark Braverman, Satyen Kale, Yury Makarychev\nUS patent application 20100198685; [USPTO]\n  Method of registering and aligning multiple images\nVyacheslav Zavadsky, Jason Abt, Mark Braverman, Edward Keyes, Vladimir Martincevic\nSemiconductor Insights Inc.\nUS patent 7,693,348 [USPTO]\n  "},{"id":12,"href":"/research/book/","title":"Book","parent":"Research","content":"Book: Computability of Julia Sets\nMark Braverman, Michael Yampolsky\nSpringer, 2009\n[Amazon] [Springer]\n"},{"id":13,"href":"/research/","title":"Research","parent":"Mark Braverman","content":""},{"id":14,"href":"/team/","title":"Group","parent":"Mark Braverman","content":""},{"id":15,"href":"/team/undergraduate-projects/","title":"Undergraduate projects","parent":"Group","content":"2015-16:  Esther Rolf \u0026lsquo;16, Thesis, \u0026ldquo;Capturing multiparty communication: extending notions of information theory to topology-dependent multiparty problems\u0026rdquo; Michael Yitayew \u0026lsquo;16, Thesis, \u0026ldquo;Short-circuit error resilience in Boolean formulas\u0026rdquo; Zhongxia (Ricky) Zhao \u0026lsquo;16, Thesis, \u0026ldquo;Non-clearing and combinatorial Walrasian equilibria\u0026rdquo; Clement Lee \u0026lsquo;17, F15, \u0026ldquo;Fast polynomial factorization\u0026rdquo; Blair Wang \u0026lsquo;17, F15, \u0026ldquo;Discovery of spurious statistical relationships\u0026rdquo;  2014-15:  David Durst \u0026lsquo;15, Thesis, \u0026ldquo;Beyond3D: Visualizing high-dimensional data sets\u0026rdquo; James Pinkerton \u0026lsquo;15, Thesis, \u0026ldquo;Energy function arguments in finding Tower of Majority lower bounds\u0026rdquo;  2013-14:  Andra Constantinescu \u0026lsquo;14, Thesis, \u0026ldquo;Behavioral mechanism design - accounting for the mental cost of choosing\u0026rdquo; Mihai Roman \u0026lsquo;14, Thesis, \u0026ldquo;Computational complexity of pricing derivatives\u0026rdquo; Leonardo Stedile \u0026lsquo;14, Thesis, \u0026ldquo;Primal-dual based weights and other kidney exchange mechanisms\u0026rdquo; Valentina Barboy \u0026lsquo;15, F13, \u0026ldquo;An exploration of the accuracy and efficiency of peer grading\u0026rdquo; Stefani Karp \u0026lsquo;15, S14, \u0026ldquo;Information-theoretic approaches to the Unexpected Hanging Paradox\u0026rdquo; Christoph Schlom \u0026lsquo;15, S14, \u0026ldquo;Calibration testing\u0026rdquo;  2012-13:  Kevin Mantel \u0026lsquo;13, Thesis, \u0026ldquo;Election manipulation through misrepresentation of pre-election polls\u0026rdquo; Gal Oshri \u0026lsquo;13, Thesis, \u0026ldquo;Contracting experts with unknown cost structures\u0026rdquo; Kanika Pasricha, Thesis, \u0026ldquo;The computational hardness of pricing compound securities\u0026rdquo; Ashish Gupta \u0026lsquo;13, F12, \u0026ldquo;An investigation of the usefulness of data preprocessing techniques versus algorithmic sophistication on diabetes prediction\u0026rdquo; Leonardo Stedile \u0026lsquo;14, F12, \u0026ldquo;Analysis of risk in chess\u0026rdquo; Xiao Tian (Elaine) Liew \u0026lsquo;14, F12, \u0026ldquo;Effect of network structure on PageRank of spam\u0026rdquo; Xin Yang Yak \u0026lsquo;14, S13, \u0026ldquo;Do YouTube users upvote Better Comments? An analysis of the influence of spelling on comment ratings\u0026rdquo;  2011-12:  Michael Zhu \u0026lsquo;13, S12, \u0026ldquo;Finding overlapping communities in graphs\u0026rdquo; Gal Oshri \u0026lsquo;13, S12, \u0026ldquo;Search on multiple indistinguishable items\u0026rdquo; Rafael Grinberg \u0026lsquo;12, S12, \u0026ldquo;The effects of noise on cellular automata\u0026rdquo;  "},{"id":16,"href":"/","title":"Mark Braverman","parent":"","content":"\nI am a professor at the Department of Computer Science at Princeton University.\nMy office is 304 in CS Building (see campus map here).\nMy brief bio can be found here.\nI am interested in complexity theory, information theory, the theory of real computation, machine learning, algorithms, algorithmic mechanism design, and its applications.\nMy e-mail address: #######@cs.princeton.edu, replacing ####### with mbraverm.\n Activities I co-organized a semester on the Nexus of Information and Computation Theories at the Institut Henri Poincaré in Paris, January–April 2016. For more information see here.\nI co-hosted a tutorial on information and communication complexity at ISIT'15 in Hong Kong.\nRecent and current program committees (since 2012): ITCS'17, FOCS'16, STOC'14, CCR'13, ITCS'13, RANDOM'12, FOCS'12.\nPapers All papers\nTeaching  Fall 2021: COS521 Advanced algorithm design  "},{"id":17,"href":"/posts/math-enabled/","title":"Math has been enabled","parent":"News","content":"Math is now enabled:\nInline math: $\\varphi = \\dfrac{1+\\sqrt5}{2}= 1.6180339887…$; or\nBlock math:\n$$ \\varphi = 1+\\frac{1} {1+\\frac{1} {1+\\frac{1} {1+\\cdots} } } $$\n"},{"id":18,"href":"/posts/new-website/","title":"New Website","parent":"News","content":"I created my new website using Hugo and GeekDocs!\n"},{"id":19,"href":"/categories/","title":"Categories","parent":"Mark Braverman","content":""},{"id":20,"href":"/tags/","title":"Tags","parent":"Mark Braverman","content":""}]